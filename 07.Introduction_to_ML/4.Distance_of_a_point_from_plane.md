# 📏 Distance of a Point from a Plane

In **Machine Learning & Geometry**, the **distance** of a point from a plane is crucial for understanding algorithms like **Logistic Regression** and **Support Vector Machines (SVMs)**.

## ✨ Understanding the Concept

A **plane** in **N-dimensional space** is represented as:

$$
\mathbf{W}^T \mathbf{X} + b = 0
$$

where:

- \( \mathbf{W} = (w_1, w_2, \dots, w_n) \) → **Normal (Weight) Vector** (always perpendicular to the plane)
- \( \mathbf{X} = (x_1, x_2, \dots, x_n) \) → **Point in N-dimensional space**
- \( b \) → **Bias term** (determines plane position)

If we have **two points**:

1. **\( S \) → \( (x_1, x_2, ..., x_n) \)**
2. **\( S' \) → \( (x'\_1, x'\_2, ..., x'\_n) \)**

we compute their distances from the plane.

---

## 📐 Distance of a Point from the Plane

The **distance** of a point **\( S \)** from the plane is given by:

$$
d = \frac{\mathbf{W}^T \mathbf{S}}{||\mathbf{W}||}
$$

Similarly, the **distance** of another point **\( S' \)** from the plane is:

$$
d' = \frac{\mathbf{W}^T \mathbf{S'}}{||\mathbf{W}||}
$$

where:

- \( ||\mathbf{W}|| = \sqrt{w_1^2 + w_2^2 + \dots + w_n^2} \) → **Magnitude (Length) of W**
- \( \mathbf{W}^T \mathbf{S} \) → **Dot Product of W and S**

---

## 📌 Interpretation Using **Angle θ**

The **dot product** formula states:

$$
\mathbf{W}^T \mathbf{S} = ||\mathbf{W}|| \cdot ||\mathbf{S}|| \cdot \cos\theta
$$

where **θ** is the angle between **W** and **S**.

### 🔹 When **\( 0^\circ \leq \theta < 90^\circ \)**

- **\( d > 0 \) (Positive Distance)**
- The point lies on the **positive side** of the plane.

### 🔹 When **\( 90^\circ < \theta < 270^\circ \)**

- **\( d < 0 \) (Negative Distance)**
- The point lies on the **negative side** of the plane.

Thus, **distance sign** helps differentiate **clusters** in classification problems! 🎯

---

## 🤖 Application in **Machine Learning**

The concept of **distance from a plane** is widely used in:

### 1️⃣ **Logistic Regression** 📊

- The **decision boundary** is a plane separating two classes.
- The **sign of the distance** determines class labels (e.g., **Positive vs. Negative**).

### 2️⃣ **Support Vector Machines (SVMs)** 🚀

- **SVMs maximize the margin** (distance) between two classes.
- The **hyperplane** is chosen to **maximize the minimum distance** between the plane and the nearest points (support vectors).

---

## 🎯 Conclusion

✅ **Distance Formula:** \( d = \frac{\mathbf{W}^T \mathbf{S}}{||\mathbf{W}||} \) helps measure how far a point is from the decision plane.  
✅ **Sign of Distance** helps **differentiate clusters** (important in classification).  
✅ **Used in Machine Learning** for **Logistic Regression, SVMs**, and other classification models.

🚀 _Understanding this concept helps in building robust ML models for classification!_
