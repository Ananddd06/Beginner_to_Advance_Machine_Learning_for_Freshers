# 📊 Step-by-Step Guide to EDA, Data Cleaning, Preprocessing, and Feature Engineering

---

## I. 🧠 Exploratory Data Analysis (EDA)

> Goal: Understand the dataset, patterns, anomalies, and relationships to inform modeling.

### 1. 🗃️ Understand the Data

- Load dataset using `pd.read_csv()` or similar.
- Use `.shape`, `.head()`, `.tail()`, `.info()`, `.describe()`.
- Check types and descriptive stats.

### 2. 🧱 Handle Missing Values

- Use `.isnull().sum()` to find nulls.
- Visualize with heatmaps (`seaborn`, `missingno`).
- Decide:
  - Drop rows/columns
  - Impute (mean/median/mode, KNN, interpolation)

### 3. 🔍 Analyze Variable Distributions

- **Numerical:** Histograms, box plots, density plots
- **Categorical:** Bar plots, count plots
- Use `.skew()` to assess skewness

### 4. 🔗 Explore Variable Relationships

- **Numerical vs. Numerical:** Correlation matrix, scatter plots
- **Numerical vs. Categorical:** Box plots, violin plots, ANOVA
- **Categorical vs. Categorical:** Cross-tab, stacked bar plots, chi-squared test

### 5. 🚨 Detect and Handle Outliers

- Use box plots, IQR, Z-score
- Decide: remove, transform, cap

### 6. 🔁 Check for Duplicates

- `.duplicated().sum()` and `.drop_duplicates()`

---

## II. 🧹 Data Cleaning

> Goal: Clean inconsistencies and prepare the dataset for modeling.

### 1. Missing Value Handling

- Apply chosen imputation (`df.fillna()`, `SimpleImputer`) or drop rows/columns.

### 2. Outlier Treatment

- Use IQR or Z-score to cap, remove, or transform extreme values.

### 3. Fix Data Type Issues

- Convert strings to numbers or datetimes using `pd.to_datetime()` or `.astype()`.

### 4. Standardize Categories

- Clean categorical typos or variations (e.g., “U.S.” vs. “USA”).

### 5. Drop Irrelevant Features

- Drop columns with high missing values, ID columns, or highly correlated features.

---

## III. ⚙️ Data Preprocessing

> Goal: Format the dataset for machine learning models.

### 1. Feature Scaling

- **StandardScaler:** Mean = 0, SD = 1
- **MinMaxScaler:** Scales between 0 and 1
- **RobustScaler:** Scales using IQR (good for outliers)

### 2. Encode Categorical Variables

- **One-Hot Encoding:** For nominal categories (`pd.get_dummies()`)
- **Label Encoding:** For ordinal variables
- **Other techniques:** Target encoding, frequency encoding

### 3. Handle Class Imbalance (For Classification)

- **SMOTE**, **ADASYN** → oversampling
- **RandomUnderSampler** → undersampling

### 4. Feature Transformation

- For skewed features: log, sqrt, Box-Cox/Yeo-Johnson
- Create polynomial terms using `PolynomialFeatures`

### 5. Data Splitting

- Use `train_test_split()` to divide data into train, test (and optionally validation)

---

## IV. 🏗️ Feature Engineering

> Goal: Enhance model performance by creating better features.

### 1. Create New Features

- From dates: Extract year, month, day
- Combine columns (e.g., ratios, diffs)
- Interaction terms (e.g., multiply features)

### 2. Polynomial Features

- Add quadratic or higher-order terms

### 3. Text Data Handling

- Tokenize, clean, and vectorize (TF-IDF, Bag of Words)
- Word embeddings (Word2Vec, GloVe)

### 4. Time Series Feature Engineering

- Lag features, rolling averages
- Extract day of week, seasonality flags, holiday indicators

### 5. Dimensionality Reduction (Optional)

- Use **PCA**, **LDA** to reduce feature space

### 6. Domain-Specific Features

- Custom ratios, scores, or external data inputs (based on problem context)

---

## 📌 Notes for Excellence

- 📊 **Visualize everything** — before and after cleaning, scaling, encoding
- 🔁 **Iterate frequently** — improvements often come from going back and refining
- 📝 **Document your work** — explain every decision in code or markdown
- 🔁 **Ensure reproducibility** — use functions/pipelines, and save preprocessors

---

## ✅ Final Deliverable for Modeling

- Cleaned, imputed, encoded, scaled, split, and engineered dataset
- Documented notebook or pipeline
- Ready to feed into your ML model
