# üìä Exploratory Data Analysis (EDA) for NLP Tasks

Exploratory Data Analysis (EDA) for NLP (Natural Language Processing) tasks focuses on understanding and preprocessing text data to build robust and intelligent models. This process helps identify patterns, anomalies, and prepare the data for feature extraction and modeling.

---

## üß≠ Step-by-Step NLP EDA Workflow

---

### 1. üóÇÔ∏è Understand the Dataset

```python
import pandas as pd

# Load data
df = pd.read_csv("dataset.csv")
df.info()
df.head()
```

- **Check for:**
  - Presence of text columns
  - Target column type (classification, sentiment, etc.)
  - Metadata or auxiliary features

---

### 2. üßº Clean the Text Data

```python
import re

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'\[.*?\]', '', text)  # remove brackets
    text = re.sub(r'https?://\S+|www\.\S+', '', text)  # remove links
    text = re.sub(r'<.*?>+', '', text)  # remove HTML tags
    text = re.sub(r'[^a-z\s]', '', text)  # remove punctuation
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\s+', ' ', text)  # remove extra spaces
    return text.strip()

df['clean_text'] = df['text'].apply(clean_text)
```

- **Actions:**
  - Lowercasing
  - Removing URLs, HTML tags, punctuations
  - Removing non-alphabet characters
  - Handling missing/blank texts

---

### 3. üîç Basic Text Analysis

```python
df['text_length'] = df['clean_text'].apply(len)
df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))
```

- **Visualizations:**

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df['word_count'], bins=30)
plt.title("Word Count Distribution")
plt.show()
```

- Check average text length and word count
- Look for outliers: very long or very short texts

---

### 4. üßæ Most Frequent Words

```python
from collections import Counter
from wordcloud import WordCloud

all_words = ' '.join(df['clean_text']).split()
most_common_words = Counter(all_words).most_common(20)

# WordCloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Most Frequent Words")
plt.show()
```

- Identify high-frequency words (remove stopwords optionally)
- Use **WordCloud** or **Bar plots**

---

### 5. ‚ùì Target-wise Text Analysis

```python
# Example: Binary Classification
df.groupby('target')['word_count'].mean().plot(kind='bar')
plt.title("Avg Word Count per Target Class")
```

- Compare text length, word usage, vocabulary richness across labels
- Great for **sentiment analysis**, **spam detection**, etc.

---

### 6. üßÆ N-grams Analysis (Bigrams / Trigrams)

```python
from sklearn.feature_extraction.text import CountVectorizer

def get_top_n_ngrams(corpus, n=None, ngram_range=(2,2)):
    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:n]

bigrams = get_top_n_ngrams(df['clean_text'], n=10, ngram_range=(2,2))
print(bigrams)
```

---

### 7. üßæ Named Entity Recognition (NER)

```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(df['clean_text'][0])

for ent in doc.ents:
    print(ent.text, ent.label_)
```

- Analyze presence of **names, places, dates**, etc.
- Helps in understanding semantic content

---

### 8. üìå Sentiment Score (Optional)

```python
from textblob import TextBlob

df['sentiment'] = df['clean_text'].apply(lambda x: TextBlob(x).sentiment.polarity)
sns.histplot(df['sentiment'], bins=30)
plt.title("Sentiment Polarity Distribution")
plt.show()
```

- Explore polarity and subjectivity
- Use for **sentiment analysis** tasks

---

### 9. üß™ Vectorization Preview (TF-IDF or Embeddings)

```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=100)
X = tfidf.fit_transform(df['clean_text'])
print(tfidf.get_feature_names_out()[:20])
```

- Preview vocabulary
- TF-IDF or CountVectorizer for shallow models
- Word2Vec, BERT, etc., for deep learning

---

### 10. ‚úÖ Final Sanity Checks

- Missing values
- Duplicates
- Class imbalance
- Unusual patterns or characters

```python
print(df['clean_text'].isnull().sum())
print(df.duplicated().sum())
print(df['target'].value_counts(normalize=True))
```

---

## üß† Bonus Tips

- Always visualize!
- Add interactivity with Plotly or Streamlit for larger reports
- Automate this workflow into a reusable notebook or script

---

## üìÅ Suggested Libraries

- `Pandas`, `Matplotlib`, `Seaborn` ‚Äì basic EDA
- `WordCloud`, `TextBlob`, `spaCy`, `NLTK` ‚Äì NLP tools
- `scikit-learn` ‚Äì vectorization and modeling

---

## üîö Conclusion

Performing EDA on NLP tasks is crucial for understanding the structure and depth of the text data. It ensures better preprocessing, modeling, and results interpretation.

Make it a habit to perform text-specific visualizations and insights before building any NLP pipeline!
