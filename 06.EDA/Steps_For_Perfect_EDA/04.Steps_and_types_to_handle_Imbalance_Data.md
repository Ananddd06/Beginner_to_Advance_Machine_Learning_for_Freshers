# Handling Imbalanced Datasets: Applied Scientist Perspective

Effectively managing class imbalance is critical to building robust, generalizable models. Below is a professional guide explaining what I follow in real-world projects.

---

## ‚úÖ Why Class Imbalance Matters

When your dataset has highly skewed class distributions (e.g., 99% vs 1%), machine learning models tend to be **biased toward the majority class**. This leads to poor recall, precision, or F1-scores, especially on the minority class. Hence, resampling techniques and class weighting become necessary to address this.

---

## ‚úÖ Scenario-Based Handling of Class Imbalance

---

### üîπ Scenario 1: **99% vs 1%** (Extreme Imbalance)

#### ‚ùå Problem:

- Model will likely ignore minority class
- SMOTE alone may cause **overfitting** in sparse regions

#### ‚úÖ Recommended Approach:

1. **SMOTE + Tomek Links or Edited Nearest Neighbors**

```python
from imblearn.combine import SMOTETomek
sm = SMOTETomek(random_state=42)
X_res, y_res = sm.fit_resample(X, y)
```

2. **Cost-Sensitive Learning**

```python
from xgboost import XGBClassifier
model = XGBClassifier(scale_pos_weight=99)
```

3. **Balanced Random Forest** (ensemble with internal class weighting)

#### ‚ö†Ô∏è Avoid:

- Random oversampling
- Naive undersampling (too much data loss)

---

### üîπ Scenario 2: **90% vs 10%** (High Imbalance)

#### ‚úÖ Recommended Approach:

1. **ADASYN or SMOTE**

```python
from imblearn.over_sampling import ADASYN
ada = ADASYN()
X_res, y_res = ada.fit_resample(X, y)
```

2. **Class Weighting**

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(class_weight='balanced')
```

3. Combine resampling + class weights and validate via Stratified K-Fold

---

### üîπ Scenario 3: **60% vs 40%** (Mild Imbalance)

#### ‚úÖ Recommended Approach:

- No need for aggressive resampling
- Use **stratified train-test split**
- Optionally use `class_weight='balanced'`

---

## ‚úÖ Summary Table

| Imbalance Ratio | Recommended Techniques                              | Reasoning                              |
| --------------- | --------------------------------------------------- | -------------------------------------- |
| 99% vs 1%       | SMOTE + Tomek, cost-sensitive learning, Balanced RF | Avoid overfitting, clean noisy borders |
| 90% vs 10%      | SMOTE/ADASYN, class weights                         | Borderline learnable, needs control    |
| 60% vs 40%      | Stratified split, optional class weights            | Mild imbalance, no need to resample    |

---

## ‚úÖ Best Practices to Avoid Overfitting/Underfitting

- üìä Use **StratifiedKFold Cross-Validation**
- üìä Track proper metrics: `precision`, `recall`, `f1-score`, `ROC-AUC`
- ‚ö†Ô∏è Never resample **test set**
- ‚úâÔ∏è Tune probability thresholds using `precision_recall_curve`

---

## üöÄ Bonus: Advanced Techniques

- **SMOTENC**: Use when dataset has both **categorical and numerical features**
- **Resampling Pipelines**:

```python
from imblearn.pipeline import Pipeline
pipeline = Pipeline([
    ('resample', SMOTETomek()),
    ('model', XGBClassifier())
])
```

- **Threshold Tuning** to optimize recall/precision trade-offs

---

## ‚úÖ Final Note from a Senior Applied Scientist

Balancing datasets isn't a one-size-fits-all. Each imbalance scenario demands careful selection of techniques, strong validation, and domain awareness. Resample wisely, monitor real-world implications, and always cross-validate.

Would you like a hands-on notebook to try these techniques?
