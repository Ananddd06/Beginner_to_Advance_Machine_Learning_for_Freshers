# 📊 Linear Regression using Ordinary Least Squares (OLS) 📉

**Linear Regression** is a fundamental statistical method used to model the relationship between an independent variable (**x**) and a dependent variable (**y**). The **Ordinary Least Squares (OLS)** method helps us find the optimal regression coefficients by minimizing the sum of squared residuals. 🎯

---

## 🔢 1. The Equation of Simple Linear Regression

The equation for **Simple Linear Regression** is:

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

Where:

- \( y \) = Target variable (dependent variable)
- \( x \) = Predictor variable (independent variable)
- \( \beta_0 \) = Intercept (bias term)
- \( \beta_1 \) = Slope (coefficient for \( x \))
- \( \epsilon \) = Error term (residuals)

Our goal is to find the best values for \( \beta_0 \) and \( \beta_1 \) such that the error is minimized. ✅

---

## 🎯 2. The Cost Function: Sum of Squared Errors (SSE)

To estimate \( \beta_0 \) and \( \beta_1 \), we use the **Sum of Squared Errors (SSE)** as our cost function:

$$
SSE = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Expanding \( \hat{y}\_i \):

$$
SSE = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2
$$

We want to find \( \beta_0 \) and \( \beta_1 \) that **minimize** this SSE. 🎯

---

## 🧮 3. Finding \( \beta_0 \) and \( \beta_1 \) Using Calculus

To minimize **SSE**, we take the **partial derivatives** of SSE with respect to \( \beta_0 \) and \( \beta_1 \) and set them equal to zero.

### 🔹 Step 1: Compute \( \frac{\partial SSE}{\partial \beta_0} \)

$$
\frac{\partial}{\partial \beta_0} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 = 0
$$

Using the **chain rule**:

$$
\sum_{i=1}^{n} -2(y_i - \beta_0 - \beta_1 x_i) = 0
$$

Rearrange:

$$
\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) = 0
$$

Divide by \( n \):

$$
\bar{y} - \beta_0 - \beta_1 \bar{x} = 0
$$

Thus, we get the formula for \( \beta_0 \) (intercept):

$$
\beta_0 = \bar{y} - \beta_1 \bar{x}
$$

Where:

- \( \bar{y} \) = Mean of **y-values**
- \( \bar{x} \) = Mean of **x-values**

---

### 🔹 Step 2: Compute \( \frac{\partial SSE}{\partial \beta_1} \)

$$
\frac{\partial}{\partial \beta_1} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 = 0
$$

Using the **chain rule**:

$$
\sum_{i=1}^{n} -2 x_i (y_i - \beta_0 - \beta_1 x_i) = 0
$$

Rearrange:

$$
\sum_{i=1}^{n} x_i y_i - \beta_0 \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 = 0
$$

Substituting \( \beta_0 = \bar{y} - \beta_1 \bar{x} \):

$$
\sum_{i=1}^{n} x_i y_i - (\bar{y} - \beta_1 \bar{x}) \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 = 0
$$

After simplifying, we obtain the formula for \( \beta_1 \) (slope):

$$
\beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
$$

This is the formula for the **slope** of the regression line. 🎉

---

## 🔑 4. Summary of OLS Formulae

After derivation, we have the final formulas for the **intercept** and **slope**:

### ✅ Formula for the **Slope** \( \beta_1 \):

$$
\beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
$$

### ✅ Formula for the **Intercept** \( \beta_0 \):

$$
\beta_0 = \bar{y} - \beta_1 \bar{x}
$$

---

## 🚀 5. Interpretation of the Coefficients

- **\( \beta_0 \) (Intercept)**: The expected value of \( y \) when \( x = 0 \).
- **\( \beta_1 \) (Slope)**: The change in \( y \) for a one-unit increase in \( x \).

**Example**: If \( \beta_1 = 2.5 \), it means that for every **1 unit increase** in \( x \), the predicted \( y \) increases by **2.5** units.

---

## ⚠️ 6. Assumptions of OLS Regression

For OLS to be **valid**, these assumptions must hold:

1. **Linearity**: The relationship between \( x \) and \( y \) is linear. 📈
2. **Independence**: Residuals (errors) are independent.
3. **Homoscedasticity**: The variance of residuals is constant. 🔍
4. **No multicollinearity**: Independent variables should not be highly correlated.
5. **Normally distributed errors**: Residuals should follow a normal distribution. 🎯

---

## 🎯 7. Key Takeaways

- **Ordinary Least Squares (OLS)** finds the best fit line by minimizing squared errors.
- **The final formulas for OLS Regression are:**
  - **Slope**:
    $$
    \beta_1 = \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
    $$
  - **Intercept**:
    $$
    \beta_0 = \bar{y} - \beta_1 \bar{x}
    $$
- **OLS assumptions** must be met for accurate results.

---

## 🔥 8. Conclusion

The **Ordinary Least Squares (OLS) method** is a powerful technique in regression analysis. By deriving the equations for **\( \beta_0 \) and \( \beta_1 \)** using calculus, we understand how OLS estimates the best-fitting line.

OLS is widely used in **machine learning**, **data science**, and **econometrics** for predictive modeling! 🚀

Happy Learning! 📊✨
