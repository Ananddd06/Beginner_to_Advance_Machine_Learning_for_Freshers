# 📉 **Cost Function in Linear Regression**

In **Linear Regression**, our goal is to find the **best-fit line** that predicts the dependent variable \( y \) based on the independent variable \( x \). To achieve this, we adjust the parameters \( \theta_0 \) (intercept) and \( \theta_1 \) (slope) to minimize the **error** between the predicted and actual values.

The **cost function** helps us quantify this error. By minimizing the cost function, we can find the line that best fits the data. Let’s break it down!

---

## 🧑‍🏫 **The Cost Function Formula:**

The cost function is defined as:

$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2
$$

Where:

- \( J(\theta_0, \theta_1) \) is the **cost function** — it measures the error.
- \( m \) is the number of training examples (data points).
- \( h\_{\theta}(x^{(i)}) = \theta_0 + \theta_1 \cdot x^{(i)} \) is the **hypothesis function** (predicted value) for each data point.
- \( y^{(i)} \) is the **actual value** of the dependent variable.

---

## ⚡ **Objective:**

- The goal is to **minimize** the cost function \( J(\theta_0, \theta_1) \) by adjusting \( \theta_0 \) and \( \theta_1 \).
- Moving towards the **global minimum** of the cost function gives us the best-fit line that minimizes the error.

---

## 🔍 **Example:**

Let’s work through a simple example with 3 data points to understand how the cost function works.

### **Given Data:**

- **x values**: [1, 2, 3]
- **y values**: [2, 3, 4]

We will initialize \( \theta_0 = 0 \) and try different values for \( \theta_1 \): \( 1 \), \( 0.5 \), and \( 0 \).

---

### **Step-by-Step Calculation:**

#### **For \( \theta_0 = 0 \) and \( \theta_1 = 1 \):**

The **hypothesis function** (predicted value) is:

$$
h_{\theta}(x) = 0 + 1 \cdot x = x
$$

So, for each data point:

- For \( x = 1 \):  
  Predicted \( h\_{\theta}(1) = 1 \), Actual \( y = 2 \)  
  **Error**: \( (1 - 2)^2 = 1 \)

- For \( x = 2 \):  
  Predicted \( h\_{\theta}(2) = 2 \), Actual \( y = 3 \)  
  **Error**: \( (2 - 3)^2 = 1 \)

- For \( x = 3 \):  
  Predicted \( h\_{\theta}(3) = 3 \), Actual \( y = 4 \)  
  **Error**: \( (3 - 4)^2 = 1 \)

Now, sum all the squared errors:

$$
J(\theta_0, \theta_1) = \frac{1}{2 \cdot 3} \left( 1 + 1 + 1 \right) = \frac{3}{6} = 0.5
$$

So, for \( \theta_0 = 0 \) and \( \theta_1 = 1 \), the total cost is **0.5**. 🎯

---

#### **For \( \theta_0 = 0 \) and \( \theta_1 = 0.5 \):**

The **hypothesis function** becomes:

$$
h_{\theta}(x) = 0 + 0.5 \cdot x = 0.5x
$$

For each data point:

- For \( x = 1 \):  
  Predicted \( h\_{\theta}(1) = 0.5 \), Actual \( y = 2 \)  
  **Error**: \( (0.5 - 2)^2 = 2.25 \)

- For \( x = 2 \):  
  Predicted \( h\_{\theta}(2) = 1 \), Actual \( y = 3 \)  
  **Error**: \( (1 - 3)^2 = 4 \)

- For \( x = 3 \):  
  Predicted \( h\_{\theta}(3) = 1.5 \), Actual \( y = 4 \)  
  **Error**: \( (1.5 - 4)^2 = 6.25 \)

Now, sum all the squared errors:

$$
J(\theta_0, \theta_1) = \frac{1}{2 \cdot 3} \left( 2.25 + 4 + 6.25 \right) = \frac{12.5}{6} = 2.0833
$$

So, for \( \theta_0 = 0 \) and \( \theta_1 = 0.5 \), the total cost is **2.0833**. 😕

---

#### **For \( \theta_0 = 0 \) and \( \theta_1 = 0 \):**

The **hypothesis function** becomes:

$$
h_{\theta}(x) = 0 + 0 \cdot x = 0
$$

For each data point:

- For \( x = 1 \):  
  Predicted \( h\_{\theta}(1) = 0 \), Actual \( y = 2 \)  
  **Error**: \( (0 - 2)^2 = 4 \)

- For \( x = 2 \):  
  Predicted \( h\_{\theta}(2) = 0 \), Actual \( y = 3 \)  
  **Error**: \( (0 - 3)^2 = 9 \)

- For \( x = 3 \):  
  Predicted \( h\_{\theta}(3) = 0 \), Actual \( y = 4 \)  
  **Error**: \( (0 - 4)^2 = 16 \)

Now, sum all the squared errors:

$$
J(\theta_0, \theta_1) = \frac{1}{2 \cdot 3} \left( 4 + 9 + 16 \right) = \frac{29}{6} = 4.8333
$$

So, for \( \theta_0 = 0 \) and \( \theta_1 = 0 \), the total cost is **4.8333**. 😬

---

## 📊 **Interpreting the Results:**

- The lowest cost occurs when \( \theta_0 = 0 \) and \( \theta_1 = 1 \), meaning this combination gives the **best-fit line**. ✅
- Larger values for \( \theta_1 \) result in higher costs, showing that the model is not a good fit for the data. 🚫

---

## 🚀 **Gradient Descent and the Global Minimum:**

In linear regression, we aim to **minimize** the cost function \( J(\theta_0, \theta_1) \), not maximize it. The **gradient descent** algorithm helps us find the **global minimum** of the cost function, which corresponds to the best-fit line.

- **Global Minimum**: The point where the cost function is at its lowest, indicating the best values for \( \theta_0 \) and \( \theta_1 \) that result in the smallest error between predicted and actual values. 🏆
- **Gradient Descent**: The process of iteratively adjusting \( \theta_0 \) and \( \theta_1 \) to reach this **global minimum**. 🔄

In simple terms, gradient descent helps us **move down the curve** to find the global minimum, which results in the **best-fit line** for the data. 🚀

---

## 🚀 **Conclusion:**

- The **cost function** allows us to **quantify the error** in our model.
- By **minimizing the cost**, we can find the best-fit line, which is the line that best represents the relationship between \( x \) and \( y \).
- In this example, we learned that adjusting the values of \( \theta_0 \) and \( \theta_1 \) helps us find the **optimal parameters** for the best-fit line. 🎯

Happy learning, and keep practicing with different values of \( \theta_0 \) and \( \theta_1 \) to better understand the process of **Linear Regression**! 😃🎉
