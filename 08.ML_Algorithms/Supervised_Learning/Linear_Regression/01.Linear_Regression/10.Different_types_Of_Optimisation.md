# Optimization Algorithms in Machine Learning

This document contains an in-depth explanation and from-scratch implementation of various optimization algorithms used in training machine learning models, especially with gradient descent. We'll use a simple **Linear Regression** example to illustrate the differences and motivations behind each method.

---

## üß† Problem Setup

We aim to minimize the **Mean Squared Error (MSE)** loss:

```
L = mean((y - (wx + b))^2)
```

Where:

- `X` is the input feature
- `y` is the true label
- `w` and `b` are model parameters (weight and bias)

We'll simulate synthetic data:

```python
# Synthetic dataset: y = 3x + 5 + noise
np.random.seed(42)
X = np.random.rand(100, 1)
y = 3 * X + 5 + np.random.randn(100, 1) * 0.1
```

---

## 1. üöÄ Vanilla Gradient Descent

**Description:**
Vanilla Gradient Descent computes the gradient using the whole dataset, and updates the parameters once per epoch.

**When to Use:**

- Small datasets
- Stable convergence required

**Limitations:**

- Slow for large datasets
- Can get stuck in local minima or saddle points

```python
def vanilla_gradient_descent(X, y, lr=0.1, epochs=100):
    w = np.random.randn(1)
    b = 0
    loss_history = []

    for epoch in range(epochs):
        y_pred = X.dot(w) + b
        dw = -2 * np.mean(X * (y - y_pred))
        db = -2 * np.mean(y - y_pred)

        w -= lr * dw
        b -= lr * db

        loss_history.append(np.mean((y - y_pred) ** 2))

    return w, b, loss_history
```

---

## 2. ‚ö° Stochastic Gradient Descent (SGD)

**Description:**
SGD updates parameters using one sample at a time, providing frequent updates.

**When to Use:**

- Large datasets
- Online learning

**Limitations:**

- Noisy convergence
- May overshoot the minimum

```python
def sgd(X, y, lr=0.1, epochs=100):
    w = np.random.randn(1)
    b = 0
    n = len(X)
    loss_history = []

    for epoch in range(epochs):
        for i in range(n):
            xi = X[i:i+1]
            yi = y[i:i+1]
            y_pred = xi.dot(w) + b

            dw = -2 * np.mean(xi * (yi - y_pred))
            db = -2 * np.mean(yi - y_pred)

            w -= lr * dw
            b -= lr * db

        loss_history.append(np.mean((y - (X.dot(w) + b)) ** 2))

    return w, b, loss_history
```

---

## 3. ‚öñÔ∏è Mini-Batch Gradient Descent

**Description:**
A compromise between Vanilla and SGD, it uses small random subsets (batches).

**When to Use:**

- Medium-to-large datasets
- GPU acceleration

```python
def mini_batch_gd(X, y, lr=0.1, epochs=100, batch_size=16):
    w = np.random.randn(1)
    b = 0
    n = len(X)
    loss_history = []

    for epoch in range(epochs):
        indices = np.random.permutation(n)
        X_shuffled = X[indices]
        y_shuffled = y[indices]

        for i in range(0, n, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            y_pred = X_batch.dot(w) + b

            dw = -2 * np.mean(X_batch * (y_batch - y_pred))
            db = -2 * np.mean(y_batch - y_pred)

            w -= lr * dw
            b -= lr * db

        loss_history.append(np.mean((y - (X.dot(w) + b)) ** 2))

    return w, b, loss_history
```

---

## 4. üåÄ Momentum

**Description:**
Accelerates SGD by adding a momentum term (like velocity in physics).

**When to Use:**

- Oscillatory gradients
- Faster convergence

```python
def momentum(X, y, lr=0.1, epochs=100, beta=0.9):
    w = np.random.randn(1)
    b = 0
    vw = 0
    vb = 0
    loss_history = []

    for epoch in range(epochs):
        y_pred = X.dot(w) + b
        dw = -2 * np.mean(X * (y - y_pred))
        db = -2 * np.mean(y - y_pred)

        vw = beta * vw + (1 - beta) * dw
        vb = beta * vb + (1 - beta) * db

        w -= lr * vw
        b -= lr * vb

        loss_history.append(np.mean((y - y_pred) ** 2))

    return w, b, loss_history
```

---

## 5. üèÉ Nesterov Accelerated Gradient (NAG)

**Description:**
Look-ahead gradient calculation improves learning dynamics over standard momentum.

**When to Use:**

- Same as momentum, but faster convergence.

```python
def nesterov(X, y, lr=0.1, epochs=100, beta=0.9):
    w = np.random.randn(1)
    b = 0
    vw = 0
    vb = 0
    loss_history = []

    for epoch in range(epochs):
        w_look = w - beta * vw
        b_look = b - beta * vb

        y_pred = X.dot(w_look) + b_look
        dw = -2 * np.mean(X * (y - y_pred))
        db = -2 * np.mean(y - y_pred)

        vw = beta * vw + lr * dw
        vb = beta * vb + lr * db

        w -= vw
        b -= vb

        loss_history.append(np.mean((y - (X.dot(w) + b)) ** 2))

    return w, b, loss_history
```

---

## 6. üß† Adagrad

**Description:**
Adaptively scales learning rate based on historical gradient squared.

**When to Use:**

- Sparse data (e.g., NLP)
- Diminishing learning rate effect

```python
def adagrad(X, y, lr=0.1, epochs=100, epsilon=1e-8):
    w = np.random.randn(1)
    b = 0
    gw = 0
    gb = 0
    loss_history = []

    for epoch in range(epochs):
        y_pred = X.dot(w) + b
        dw = -2 * np.mean(X * (y - y_pred))
        db = -2 * np.mean(y - y_pred)

        gw += dw**2
        gb += db**2

        w -= lr * dw / (np.sqrt(gw) + epsilon)
        b -= lr * db / (np.sqrt(gb) + epsilon)

        loss_history.append(np.mean((y - y_pred) ** 2))

    return w, b, loss_history
```

---

## 7. üìâ RMSprop

**Description:**
Like Adagrad, but uses exponential decay on past gradients.

**When to Use:**

- Non-stationary objectives
- RNNs

```python
def rmsprop(X, y, lr=0.01, epochs=100, beta=0.9, epsilon=1e-8):
    w = np.random.randn(1)
    b = 0
    sw = 0
    sb = 0
    loss_history = []

    for epoch in range(epochs):
        y_pred = X.dot(w) + b
        dw = -2 * np.mean(X * (y - y_pred))
        db = -2 * np.mean(y - y_pred)

        sw = beta * sw + (1 - beta) * dw**2
        sb = beta * sb + (1 - beta) * db**2

        w -= lr * dw / (np.sqrt(sw) + epsilon)
        b -= lr * db / (np.sqrt(sb) + epsilon)

        loss_history.append(np.mean((y - y_pred) ** 2))

    return w, b, loss_history
```

---

## 8. üîÆ Adam Optimizer

**Description:**
Combines momentum and RMSprop. Currently the most popular optimizer.

**When to Use:**

- Deep learning
- Sparse gradients or noisy data

```python
def adam(X, y, lr=0.01, epochs=100, beta1=0.9, beta2=0.999, epsilon=1e-8):
    w = np.random.randn(1)
    b = 0
    mw = 0
    vw = 0
    mb = 0
    vb = 0
    loss_history = []

    for epoch in range(1, epochs + 1):
        y_pred = X.dot(w) + b
        dw = -2 * np.mean(X * (y - y_pred))
        db = -2 * np.mean(y - y_pred)

        mw = beta1 * mw + (1 - beta1) * dw
        vw = beta2 * vw + (1 - beta2) * dw**2
        mb = beta1 * mb + (1 - beta1) * db
        vb = beta2 * vb + (1 - beta2) * db**2

        # Bias correction
        mw_hat = mw / (1 - beta1**epoch)
        vw_hat = vw / (1 - beta2**epoch)
        mb_hat = mb / (1 - beta1**epoch)
        vb_hat = vb / (1 - beta2**epoch)

        w -= lr * mw_hat / (np.sqrt(vw_hat) + epsilon)
        b -= lr * mb_hat / (np.sqrt(vb_hat) + epsilon)

        loss_history.append(np.mean((y - y_pred) ** 2))

    return w, b, loss_history
```

---

### üîÅ Bonus: Adam + Early Stopping

You can add an **early stopping** mechanism by monitoring loss history and stopping if the improvement becomes too small.

---

## üìä Visual Comparison

To compare all optimizers, plot their `loss_history` curves.

```python
plt.plot(loss_vanilla, label="Vanilla GD")
plt.plot(loss_sgd, label="SGD")
plt.plot(loss_momentum, label="Momentum")
plt.plot(loss_adam, label="Adam")
plt.legend()
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Optimizer Comparison")
plt.show()
```

---

## üìå Conclusion

| Optimizer  | Best Use Case                  |
| ---------- | ------------------------------ |
| Vanilla GD | Small datasets                 |
| SGD        | Online learning, fast feedback |
| Mini-Batch | Scalable, GPU-compatible       |
| Momentum   | Smooth convergence             |
| Nesterov   | Faster than Momentum           |
| Adagrad    | Sparse data                    |
| RMSprop    | RNNs, noisy problems           |
| Adam       | Deep Learning, default choice  |

---

Let me know if you want a Jupyter notebook or want to integrate this into a GitHub project!
