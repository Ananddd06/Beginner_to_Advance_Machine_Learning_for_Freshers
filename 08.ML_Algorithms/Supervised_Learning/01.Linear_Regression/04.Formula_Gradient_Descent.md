# Gradient Descent for Linear Regression

The update rule for Gradient Descent is:

$$
\theta_j = \theta_j - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta_j}
$$

where:

- \( \theta_j \) is the parameter (weight or bias) we want to update.
- \( \alpha \) is the learning rate (step size).
- \( J(\theta) \) is the cost function (Mean Squared Error in Linear Regression).
- \( \frac{\partial J(\theta)}{\partial \theta_j} \) is the gradient of the cost function with respect to \( \theta_j \).

---

## Gradient Descent Update for Linear Regression

For **Linear Regression**, the cost function is:

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x_i) - y_i)^2
$$

where:

- \( h\_{\theta}(x) = \theta_0 + \theta_1 x \) (hypothesis function)
- \( m \) is the number of training examples.

The **partial derivatives (gradients)** are:

$$
\frac{\partial J}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x_i) - y_i)
$$

$$
\frac{\partial J}{\partial \theta_1} = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x_i) - y_i) x_i
$$

So the **parameter updates** are:

$$
\theta_0 = \theta_0 - \alpha \cdot \frac{1}{m} \sum (h_{\theta}(x) - y)
$$

$$
\theta_1 = \theta_1 - \alpha \cdot \frac{1}{m} \sum (h_{\theta}(x) - y) \cdot x
$$

- PYTHON CODE FOR THAT

```python

import numpy as np
import matplotlib.pyplot as plt

# Generate Random Data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# Compute Cost Function
def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    errors = predictions - y
    cost = (1 / (2 * m)) * np.sum(errors ** 2)
    return cost

# Gradient Descent Function
def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    cost_history = []

    for _ in range(iterations):
        predictions = X.dot(theta)
        errors = predictions - y
        gradients = (1 / m) * X.T.dot(errors)
        theta -= learning_rate * gradients

        cost = compute_cost(X, y, theta)
        cost_history.append(cost)

    return theta, cost_history

# Add Bias Term to X
X_b = np.c_[np.ones((100, 1)), X]

# Initialize Parameters
theta_initial = np.random.randn(2, 1)

# Set Hyperparameters
learning_rate = 0.1
iterations = 1000

# Run Gradient Descent
theta_optimal, cost_history = gradient_descent(X_b, y, theta_initial, learning_rate, iterations)

# Plot Cost Function
plt.plot(range(iterations), cost_history, 'b-')
plt.xlabel("Iterations")
plt.ylabel("Cost")
plt.title("Cost Function over Iterations")
plt.show()

# Plot Best-Fit Line
plt.scatter(X, y, color='blue', label='Data points')
plt.plot(X, X_b.dot(theta_optimal), color='red', linewidth=2, label='Best Fit Line')
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear Regression using Gradient Descent")
plt.legend()
plt.show()

# Print Final Parameters
print("Optimal Theta (Parameters):")
print(theta_optimal)

```
