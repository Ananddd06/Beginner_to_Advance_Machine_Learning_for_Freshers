# 🧪 Confidence Intervals & Margin of Error

## 📌 Introduction

In inferential statistics, we use **confidence intervals (CIs)** and **margins of error (MoE)** to **quantify uncertainty** in estimates. When we collect a sample and want to say something about the population, we're always operating with **some degree of uncertainty**.

Understanding CIs and MoE is crucial for building reliable models, designing robust A/B tests, interpreting results, and driving decisions in research and production systems.

---

## 🎯 What is a Confidence Interval?

A **confidence interval** gives a range of plausible values for an unknown **population parameter** (like mean or proportion), based on sample data.

### 💡 Intuition

> Instead of saying "the average session time is 5.2 minutes," we say:  
> 🔍 “We are 95% confident that the true mean session time is between 4.8 and 5.6 minutes.”

### 🧠 Formal Definition

A **confidence interval** is:

$$
\text{CI} = \bar{x} \pm Z^* \cdot \frac{\sigma}{\sqrt{n}} \quad \text{(Z-Test)} \\
\text{or} \\
\text{CI} = \bar{x} \pm t^* \cdot \frac{s}{\sqrt{n}} \quad \text{(T-Test)}
$$

Where:

| Symbol         | Meaning                                   |
| -------------- | ----------------------------------------- |
| $\bar{x}$      | Sample Mean                               |
| $Z^*$ / $t^*$  | Critical value (from Z or T distribution) |
| $\sigma$ / $s$ | Population or sample standard deviation   |
| $n$            | Sample size                               |

---

## 🔍 What is the Margin of Error?

The **Margin of Error (MoE)** is the amount added and subtracted from the sample estimate to create the interval.

### 🔧 Formula:

$$
\text{MoE} = Z^* \cdot \frac{\sigma}{\sqrt{n}} \quad \text{(or)} \quad t^* \cdot \frac{s}{\sqrt{n}}
$$

### 📌 Interpretation:

- Wider intervals = More uncertainty
- Narrower intervals = More precision

🔁 **CI = Estimate ± MoE**

---

## 📊 Choosing Confidence Level

The **confidence level** (e.g., 90%, 95%, 99%) indicates how confident we are that the interval contains the true parameter.

| Confidence Level | Z* or t* Value |
| ---------------- | -------------- |
| 90%              | 1.645          |
| 95%              | 1.96           |
| 99%              | 2.576          |

**Higher confidence ⇒ Wider interval**

---

## 🛠️ Use Case: A/B Testing Example

Imagine you're testing two versions of a UI:

- Variant A mean session: 5.2 mins
- Variant B mean session: 5.6 mins
- $s = 0.4$, $n = 50$, use 95% confidence

### Step 1: Calculate MoE for Variant A

$$
MoE = 1.96 \cdot \frac{0.4}{\sqrt{50}} = 1.96 \cdot 0.0565 \approx 0.11
$$

### Step 2: Construct CI

$$
CI = 5.2 \pm 0.11 = [5.09, 5.31]
$$

Same for Variant B:  
CI = [5.49, 5.71]

Since the intervals **do not overlap**, we can infer a **statistically significant** difference.

---

## Confidence Interval Calculation using the BootStrap Tool in Python with the Graph Representation

```python

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import bootstrap

# Sample data
data = np.array([10, 12, 13, 15, 18, 21, 25, 30, 35, 40])

# Reshape data for bootstrap
data_for_bootstrap = (data,)

# Run bootstrap (95% CI for mean)
res = bootstrap(
    data_for_bootstrap,
    np.mean,
    confidence_level=0.95,
    n_resamples=1000,
    method='percentile',
    random_state=42
)

# CI bounds
ci_low = res.confidence_interval.low
ci_high = res.confidence_interval.high

# Generate bootstrap sample distribution for plotting
boot_means = []
for _ in range(1000):
    sample = np.random.choice(data, size=len(data), replace=True)
    boot_means.append(np.mean(sample))

# Plot
plt.figure(figsize=(10, 6))
plt.hist(boot_means, bins=30, color='skyblue', edgecolor='black')
plt.axvline(ci_low, color='red', linestyle='--', label=f'Lower CI = {ci_low:.2f}')
plt.axvline(ci_high, color='green', linestyle='--', label=f'Upper CI = {ci_high:.2f}')
plt.axvline(np.mean(data), color='purple', linestyle='-', label=f'Mean = {np.mean(data):.2f}')
plt.title('Bootstrap Sampling Distribution of the Mean')
plt.xlabel('Mean values from bootstrap samples')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True)
plt.show()

```

![Bootstrap Sampling Distribution](Insight_Images/output.png)

## ⚠️ Common Misconceptions

### ❌ "95% of data falls in the interval."

✅ The correct interpretation: _If we repeated the experiment 100 times, ~95 of those CIs would contain the true mean._

### ❌ "We are 95% sure the parameter is in this interval."

✅ Once the interval is calculated, it **either contains the parameter or it doesn't**. The **confidence** is in the _process_, not the _specific interval_.

---

## 📐 Impact of Sample Size

- 👇 Small `n` = larger standard error → wider CI
- 🔼 Large `n` = smaller standard error → tighter CI

### Example:

- $n = 25$: CI = [5.0, 6.0]
- $n = 100$: CI = [5.2, 5.8]

---

## 🧮 T vs Z: Which One?

| Criteria                     | Use Z-Test           | Use T-Test           |
| ---------------------------- | -------------------- | -------------------- |
| Population std dev known     | ✅ Yes               | ❌ No                |
| Sample size large ($n > 30$) | ✅ Yes (CLT applies) | ✅ Often still use T |
| Sample size small ($n < 30$) | ❌ No                | ✅ Use T             |

---

## 🧩 Confidence Interval for Proportions

For estimating proportions (e.g., 65% of users clicked), we use:

$$
CI = \hat{p} \pm Z^* \cdot \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}
$$

---

## ✅ Key Takeaways

- **Confidence Intervals** express uncertainty in estimation.
- The **Margin of Error** reflects the radius of this uncertainty.
- Higher confidence ⇒ wider interval.
- Larger sample size ⇒ narrower interval.
- Use **Z** when $\sigma$ is known or $n$ is large, else use **T**.

---

## 🔚 Conclusion

Understanding confidence intervals is foundational for scientific thinking. In applied science roles, it allows us to:

- Make **informed decisions** under uncertainty,
- **Quantify model stability** and reliability,
- Present **transparent and reproducible** results.

> “Statistics is the grammar of science.” — Karl Pearson

---
