# üìä Guide to Parametric, Non-Parametric, and Categorical Variable Tests with Code

This guide provides a comprehensive understanding of various types of statistical significance tests, categorized into Parametric, Non-Parametric, and Categorical tests. Each test is explained with real-world context, when to use it, and Python code implementation.

---

## üîπ A. Parametric Tests

Parametric tests assume that the data follows a known distribution, typically the **normal distribution**. They are powerful when assumptions hold true but may mislead when assumptions are violated.

### 1. **t-test**

- **Use Case**: Compare means of two independent groups
- **Example**: Exam scores of males vs. females
- **When to Use**:

  - Data is **numerical and continuous**
  - Groups are **independent**
  - Data is **normally distributed**

- **Code**:

```python
from scipy.stats import ttest_ind
import numpy as np

group_a = np.random.normal(70, 10, 30)
group_b = np.random.normal(75, 12, 30)

stat, p = ttest_ind(group_a, group_b)
print(f"T-statistic: {stat:.3f}, P-value: {p:.3f}")
```

---

### 2. **Paired t-test**

- **Use Case**: Compare means from the **same group** at two time points
- **Example**: Blood pressure before and after treatment
- **When to Use**:

  - Measurements are **paired** (same individuals)
  - Normal distribution of differences

- **Code**:

```python
from scipy.stats import ttest_rel

before = np.random.normal(120, 10, 30)
after = before - np.random.normal(5, 3, 30)

stat, p = ttest_rel(before, after)
print(f"Paired T-statistic: {stat:.3f}, P-value: {p:.3f}")
```

---

### 3. **Z-test**

- **Use Case**: Compare mean of sample to population when variance is known or sample size is large
- **Example**: Test if sample avg height differs from national avg
- **When to Use**:

  - Large sample size (n > 30) or known variance

- **Code**:

```python
from statsmodels.stats.weightstats import ztest

sample = np.random.normal(65, 5, 100)
stat, p = ztest(sample, value=64)
print(f"Z-statistic: {stat:.3f}, P-value: {p:.3f}")
```

---

### 4. **ANOVA**

- **Use Case**: Compare means of **3 or more** independent groups
- **Example**: Scores from three teaching methods
- **When to Use**:

  - Numerical data
  - 3+ independent groups

- **Code**:

```python
from scipy.stats import f_oneway

group1 = np.random.normal(70, 10, 30)
group2 = np.random.normal(75, 10, 30)
group3 = np.random.normal(72, 10, 30)

stat, p = f_oneway(group1, group2, group3)
print(f"ANOVA F-statistic: {stat:.3f}, P-value: {p:.3f}")
```

---

## üîπ B. Non-Parametric Tests

Used when data **doesn't follow a normal distribution** or is **ordinal**.

### 1. **Mann-Whitney U Test**

- **Use Case**: Compare medians of two independent groups
- **Example**: Ratings from two products
- **When to Use**:

  - Ordinal or skewed continuous data

- **Code**:

```python
from scipy.stats import mannwhitneyu

a = np.random.exponential(scale=2, size=30)
b = np.random.exponential(scale=2.5, size=30)

stat, p = mannwhitneyu(a, b)
print(f"Mann-Whitney U: {stat:.3f}, P-value: {p:.3f}")
```

---

### 2. **Wilcoxon Signed Rank Test**

- **Use Case**: Paired non-parametric test (e.g., pre-post score comparison)
- **When to Use**:

  - Differences are **not normally distributed**

- **Code**:

```python
from scipy.stats import wilcoxon

before = np.random.exponential(2, 30)
after = before - np.random.normal(1, 0.5, 30)

stat, p = wilcoxon(before, after)
print(f"Wilcoxon test: {stat:.3f}, P-value: {p:.3f}")
```

---

### 3. **Kruskal-Wallis Test**

- **Use Case**: Non-parametric ANOVA
- **Example**: Ratings from 3+ groups with non-normal data
- **Code**:

```python
from scipy.stats import kruskal

g1 = np.random.exponential(2, 30)
g2 = np.random.exponential(2.5, 30)
g3 = np.random.exponential(3, 30)

stat, p = kruskal(g1, g2, g3)
print(f"Kruskal-Wallis H: {stat:.3f}, P-value: {p:.3f}")
```

---

### 4. **Permutation Test**

- **Use Case**: Compare two groups by label shuffling
- **Code**:

```python
import numpy as np

def permutation_test(a, b, n_permutations=10000):
    observed_diff = np.mean(a) - np.mean(b)
    combined = np.concatenate([a, b])
    count = 0
    for _ in range(n_permutations):
        np.random.shuffle(combined)
        new_a = combined[:len(a)]
        new_b = combined[len(a):]
        diff = np.mean(new_a) - np.mean(new_b)
        if abs(diff) >= abs(observed_diff):
            count += 1
    return observed_diff, count / n_permutations

a = np.random.normal(10, 1.5, 25)
b = np.random.normal(11.2, 1.5, 25)
obs_diff, p = permutation_test(a, b)
print(f"Observed diff: {obs_diff:.3f}, Permutation P-value: {p:.3f}")
```

---

### 5. **Bootstrap Test**

- **Use Case**: Estimate distribution of any statistic via resampling
- **Code**:

```python
import numpy as np

def bootstrap(data, n_bootstrap=10000):
    means = []
    for _ in range(n_bootstrap):
        sample = np.random.choice(data, size=len(data), replace=True)
        means.append(np.mean(sample))
    return np.percentile(means, [2.5, 97.5])

sample = np.random.normal(10, 2, 100)
ci = bootstrap(sample)
print(f"95% CI for mean: {ci}")
```

---

## üîπ C. Categorical Variable Tests

### 1. **Chi-square Test of Independence**

- **Use Case**: Association between 2 categorical variables
- **Code**:

```python
from scipy.stats import chi2_contingency
import numpy as np

table = np.array([[20, 15], [30, 35]])
chi2, p, dof, expected = chi2_contingency(table)
print(f"Chi-square: {chi2:.3f}, P-value: {p:.3f}")
```

---

### 2. **Fisher's Exact Test**

- **Use Case**: Small-sample test of independence
- **Code**:

```python
from scipy.stats import fisher_exact

table = [[5, 1], [2, 7]]
stat, p = fisher_exact(table)
print(f"Fisher Exact Test P-value: {p:.3f}")
```

---

### 3. **Proportion Z-Test**

- **Use Case**: Compare proportions in 2 groups
- **Code**:

```python
from statsmodels.stats.proportion import proportions_ztest

success = [45, 30]
total = [100, 100]
stat, p = proportions_ztest(success, total)
print(f"Proportion Z-test: {stat:.3f}, P-value: {p:.3f}")
```

---

## üìÅ Summary

Use these tests to evaluate the statistical significance of differences, associations, or estimations across datasets in real-world scenarios such as ML experiments, A/B testing, and product analysis.

Each test has different assumptions and ideal use cases‚Äîchoose based on **data type**, **sample size**, and **distribution shape**.

Would you like this compiled into a Jupyter Notebook or HTML file for hands-on learning?
