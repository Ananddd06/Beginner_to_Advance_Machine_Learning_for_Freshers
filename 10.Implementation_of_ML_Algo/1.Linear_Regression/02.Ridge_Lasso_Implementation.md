# **Ridge, Lasso, and Elastic Net Regression from Scratch** üìä

## **Introduction** ü§î

In this tutorial, we will implement **Ridge Regression**, **Lasso Regression**, and **Elastic Net Regression** from scratch using **numpy**. These are regularization techniques that help to prevent overfitting in linear regression models by adding a penalty to the model's complexity.

### **Key Concepts** üß†

- **Ridge Regression** (L2 Regularization) penalizes large coefficients by adding a squared penalty term.
- **Lasso Regression** (L1 Regularization) penalizes the absolute values of the coefficients, which can set some coefficients to zero.
- **Elastic Net Regression** is a combination of **L1** and **L2** regularization, providing the flexibility to balance between Ridge and Lasso.

---

## **Ridge Regression (L2 Regularization)** üèîÔ∏è

### **Cost Function**:

The cost function for **Ridge Regression** is:

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (y_i - X_i \theta)^2 + \lambda \sum_{j=1}^{p} \theta_j^2
$$

Where:

- \( y_i \) is the actual value.
- \( X_i \) is the input features.
- \( \theta_j \) are the model coefficients.
- \( \lambda \) is the regularization parameter that controls the magnitude of the penalty.

The **L2** penalty term \( \lambda \sum\_{j=1}^{p} \theta_j^2 \) shrinks the coefficients, which helps in **preventing overfitting** by reducing model complexity.

---

## **Lasso Regression (L1 Regularization)** üöÄ

### **Cost Function**:

The cost function for **Lasso Regression** is:

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (y_i - X_i \theta)^2 + \lambda \sum_{j=1}^{p} |\theta_j|
$$

Where:

- \( \lambda \) is the regularization parameter.
- \( |\theta_j| \) is the **absolute value** of the coefficients.

The **L1** penalty term \( \lambda \sum\_{j=1}^{p} |\theta_j| \) encourages **sparsity** in the coefficients. This means that **Lasso can eliminate features** by setting some coefficients to zero, making it useful for **feature selection**.

---

## **Elastic Net Regression** ‚öñÔ∏è

### **Cost Function**:

The **Elastic Net** cost function combines both **L1** and **L2** penalties:

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (y_i - X_i \theta)^2 + \lambda \left( \sum_{j=1}^{p} \theta_j^2 + \sum_{j=1}^{p} |\theta_j| \right)
$$

Where:

- \( \alpha \) controls the balance between **L1** and **L2** regularization.
  - \( \alpha = 1 \) corresponds to **Lasso Regression** (pure L1).
  - \( \alpha = 0 \) corresponds to **Ridge Regression** (pure L2).

Elastic Net is particularly useful when there are **correlated features**.

---

## **Relationship Between \( \lambda \) and the Slope (Coefficients)** üîÑ

1. **Ridge Regression**: As \( \lambda \) increases, the coefficients \( \theta_j \) shrink, but they never become exactly zero. This helps reduce the model's complexity without losing features.

2. **Lasso Regression**: As \( \lambda \) increases, **some coefficients are forced to zero**. This effectively eliminates less important features, making Lasso great for **feature selection**.

3. **Elastic Net**: The value of \( \alpha \) determines how much **L1 (Lasso)** or **L2 (Ridge)** regularization is applied. When \( \alpha = 1 \), it behaves like Lasso, and when \( \alpha = 0 \), it behaves like Ridge.

---

## **Gradient Descent for Optimization** ‚¨áÔ∏è

In all three regression techniques, we use **Gradient Descent** to minimize the cost function. The gradient of the cost function is computed, and the coefficients are updated iteratively until convergence. The regularization term ensures that the coefficients don‚Äôt become too large and the model generalizes better.

---

## **Code Implementation** üë©‚Äçüíªüë®‚Äçüíª

Here‚Äôs the code implementing **Ridge**, **Lasso**, and **Elastic Net** regression from scratch using numpy:

```python
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic dataset: 100 samples, 4 features
np.random.seed(42)
X = np.random.rand(100, 4)  # Features: location, education, experience, gender (encoded)
true_weights = np.array([2.5, -1.2, 3.0, 0.5])  # Just for simulation
y = X.dot(true_weights) + 1 + np.random.randn(100)  # Add intercept and some noise

# Add bias (intercept) term
def add_bias(X):
    m = X.shape[0]
    return np.c_[np.ones((m, 1)), X]

# Ridge Cost Function
def compute_ridge_cost(X_b, y, theta, alpha):
    m = len(y)
    predictions = X_b.dot(theta)
    error = predictions - y
    cost = (1/(2*m)) * np.sum(error**2) + alpha * np.sum(theta[1:]**2)
    return cost

# Ridge Regression Implementation
def ridge_regression(X, y, alpha=1.0, num_iters=1000, learning_rate=0.01):
    m, n = X.shape
    X_b = add_bias(X)
    theta = np.zeros(n + 1)
    costs = []

    for i in range(num_iters):
        gradients = (2/m) * X_b.T.dot(X_b.dot(theta) - y)
        reg_term = 2 * alpha * np.r_[[[0]], theta[1:].reshape(-1, 1)]
        gradients += reg_term.flatten()
        theta -= learning_rate * gradients
        costs.append(compute_ridge_cost(X_b, y, theta, alpha))
    return theta, costs

# Lasso Cost Function
def compute_lasso_cost(X_b, y, theta, alpha):
    m = len(y)
    predictions = X_b.dot(theta)
    error = predictions - y
    cost = (1/(2*m)) * np.sum(error**2) + alpha * np.sum(np.abs(theta[1:]))
    return cost

# Lasso Regression Implementation
def lasso_regression(X, y, alpha=0.1, num_iters=1000, learning_rate=0.01):
    m, n = X.shape
    X_b = add_bias(X)
    theta = np.zeros(n + 1)
    costs = []

    for i in range(num_iters):
        gradients = (2/m) * X_b.T.dot(X_b.dot(theta) - y)
        reg_term = alpha * np.r_[[[0]], np.sign(theta[1:]).reshape(-1, 1)]
        gradients += reg_term.flatten()
        theta -= learning_rate * gradients
        costs.append(compute_lasso_cost(X_b, y, theta, alpha))
    return theta, costs

# Run Ridge
alpha_ridge = 1.0
theta_ridge, cost_ridge = ridge_regression(X, y, alpha=alpha_ridge)
print(f"Ridge Coefficients: {theta_ridge}")

# Run Lasso
alpha_lasso = 0.1
theta_lasso, cost_lasso = lasso_regression(X, y, alpha=alpha_lasso)
print(f"Lasso Coefficients: {theta_lasso}")

# Plot cost over iterations
plt.figure(figsize=(10, 5))
plt.plot(cost_ridge, label=f'Ridge (alpha={alpha_ridge})')
plt.plot(cost_lasso, label=f'Lasso (alpha={alpha_lasso})')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.title('Cost Function Convergence')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

```

### **Implementing Ridge, Lasso, and Elastic Net with Scikit-learn** üìä

You can easily implement **Ridge**, **Lasso**, and **Elastic Net** using the powerful machine learning library, **Scikit-learn**. Below is the Python code to perform these regression techniques using the built-in functions from `sklearn`.

### **1. Ridge Regression** üîµ

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression

# Generate a simple regression dataset
X, y = make_regression(n_samples=100, n_features=3, noise=0.1)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Ridge regression model
ridge = Ridge(alpha=1.0)  # Set the regularization parameter alpha
ridge.fit(X_train, y_train)

# Get the model's coefficients and intercept
print("Ridge Coefficients:", ridge.coef_)
print("Ridge Intercept:", ridge.intercept_)

# Evaluate the model on the test data
ridge_score = ridge.score(X_test, y_test)
print("Ridge Regression R^2 Score:", ridge_score)

```

### **2. Lasso Regression** üî¥

```python
from sklearn.linear_model import Lasso

# Create and train the Lasso regression model
lasso = Lasso(alpha=0.1)  # Set the regularization parameter alpha
lasso.fit(X_train, y_train)

# Get the model's coefficients and intercept
print("Lasso Coefficients:", lasso.coef_)
print("Lasso Intercept:", lasso.intercept_)

# Evaluate the model on the test data
lasso_score = lasso.score(X_test, y_test)
print("Lasso Regression R^2 Score:", lasso_score)

```

### **3. Elastic Net Regression** üîµüî¥

```python
from sklearn.linear_model import ElasticNet

# Create and train the Elastic Net regression model
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.7)  # Set the regularization parameter alpha and l1_ratio
elastic_net.fit(X_train, y_train)

# Get the model's coefficients and intercept
print("Elastic Net Coefficients:", elastic_net.coef_)
print("Elastic Net Intercept:", elastic_net.intercept_)

# Evaluate the model on the test data
elastic_net_score = elastic_net.score(X_test, y_test)
print("Elastic Net Regression R^2 Score:", elastic_net_score)

```

# **Explanation of Code** üìù

### **Ridge Regression**:

- We use the `Ridge` class from `sklearn.linear_model` and specify the regularization parameter `alpha`.
- The model is trained using `.fit()` on the training data.
- The coefficients and intercept are retrieved using `.coef_` and `.intercept_`.
- The performance is evaluated using `.score()`, which returns the R¬≤ score (the proportion of variance explained by the model).

### **Lasso Regression**:

- Similar to Ridge, but using the `Lasso` class from `sklearn.linear_model`.
- The `alpha` parameter controls the amount of regularization.
- Lasso can zero out some of the coefficients, effectively performing **feature selection**.

### **Elastic Net Regression**:

- The `ElasticNet` class from `sklearn.linear_model` is used.
- The `alpha` parameter controls the overall strength of regularization.
- The `l1_ratio` parameter controls the balance between **L1** (Lasso) and **L2** (Ridge) penalties. A value of `l1_ratio=0.7` means 70% of L1 penalty and 30% of L2 penalty.

# **Conclusion** üéØ

- **Ridge Regression** helps in shrinking coefficients to avoid overfitting, especially when the features are highly correlated.
- **Lasso Regression** helps in feature selection by forcing some coefficients to zero.
- **Elastic Net** combines both **L1** and **L2** penalties, making it flexible for datasets with correlated features.

By using these techniques appropriately, you can improve your model‚Äôs generalization ability and handle overfitting effectively. üöÄ

This explanation provides a clear and structured overview of **Ridge**, **Lasso**, and **Elastic Net** regression, along with their cost functions, gradient descent implementation, and the relationship between **Œª (lambda)** and the coefficients.
