## For Linear Regression

```python

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, r2_score

# Hyperparameter Tuning for Linear Regression
learning_rates = [0.001, 0.01, 0.1, 0.5, 1]  # Different learning rates
best_lr = None
best_mse = float('inf')
best_weights = None

# Loop over different learning rates
for lr in learning_rates:
    # Initialize the model
    y_train = y_train.reshape(-1, 1)
    m, n = X_train_Scaled.shape
    X_b = np.c_[np.ones((m, 1)), X_train_Scaled]
    weights = np.zeros((n + 1, 1))

    n_epochs = 1000

    # Gradient Descent
    for epoch in range(n_epochs):
        y_pred = X_b.dot(weights)
        error = y_pred - y_train
        gradients = (2/m) * X_b.T.dot(error)
        weights -= lr * gradients

    # Evaluate the model with MSE (Mean Squared Error)
    y_pred_test = X_test_Scaled.dot(weights[1:]) + weights[0]
    mse = np.mean((y_pred_test - y_test) ** 2)

    if mse < best_mse:
        best_mse = mse
        best_lr = lr
        best_weights = weights

print(f"Best Learning Rate: {best_lr} with MSE: {best_mse:.4f}")

# Test prediction and evaluation on the best model
y_pred_test = X_test_Scaled.dot(best_weights[1:]) + best_weights[0]
mae = mean_absolute_error(y_test, y_pred_test)
r2 = r2_score(y_test, y_pred_test)

print(f"Mean Absolute Error (Test): {mae:.4f}")
print(f"R2 Score (Test): {r2:.4f}")

```

## For Ridge Regression

```python

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, r2_score

# Hyperparameter Tuning for Ridge Regression
learning_rates = [0.001, 0.01, 0.1, 0.5, 1]  # Different learning rates
lambda_vals = [0.01, 0.1, 1, 10]  # Different regularization strengths (L2 penalty)
best_lr = None
best_lambda = None
best_mse = float('inf')
best_weights = None

# Loop over learning rates and lambda values
for lr in learning_rates:
    for lambda_reg in lambda_vals:
        # Initialize model
        y_train = y_train.reshape(-1, 1)
        m, n = X_train_Scaled.shape
        X_b = np.c_[np.ones((m, 1)), X_train_Scaled]
        weights = np.zeros((n + 1, 1))

        n_epochs = 1000

        # Gradient Descent for Ridge Regression
        for epoch in range(n_epochs):
            y_pred = X_b.dot(weights)
            error = y_pred - y_train
            gradients = (2/m) * X_b.T.dot(error) + (2 * lambda_reg * weights)  # L2 penalty
            weights -= lr * gradients

        # Evaluate with MSE
        y_pred_test = X_test_Scaled.dot(weights[1:]) + weights[0]
        mse = np.mean((y_pred_test - y_test) ** 2)

        if mse < best_mse:
            best_mse = mse
            best_lr = lr
            best_lambda = lambda_reg
            best_weights = weights

print(f"Best Learning Rate: {best_lr}, Best Lambda: {best_lambda} with MSE: {best_mse:.4f}")

# Test prediction and evaluation on the best model
y_pred_test = X_test_Scaled.dot(best_weights[1:]) + best_weights[0]
mae = mean_absolute_error(y_test, y_pred_test)
r2 = r2_score(y_test, y_pred_test)

print(f"Mean Absolute Error (Test): {mae:.4f}")
print(f"R2 Score (Test): {r2:.4f}")

```

## For Lasso Regression

```python

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, r2_score

# Hyperparameter Tuning for Lasso Regression
learning_rates = [0.001, 0.01, 0.1, 0.5, 1]  # Different learning rates
lambda_vals = [0.01, 0.1, 1, 10]  # Different regularization strengths (L1 penalty)
best_lr = None
best_lambda = None
best_mse = float('inf')
best_weights = None

# Loop over learning rates and lambda values
for lr in learning_rates:
    for lambda_reg in lambda_vals:
        # Initialize model
        y_train = y_train.reshape(-1, 1)
        m, n = X_train_Scaled.shape
        X_b = np.c_[np.ones((m, 1)), X_train_Scaled]
        weights = np.zeros((n + 1, 1))

        n_epochs = 1000

        # Gradient Descent for Lasso Regression
        for epoch in range(n_epochs):
            y_pred = X_b.dot(weights)
            error = y_pred - y_train
            gradients = (2/m) * X_b.T.dot(error)

            # L1 Regularization part (shrinkage for Lasso)
            for j in range(1, n+1):  # Skip bias term
                if weights[j] > 0:
                    weights[j] -= lambda_reg
                elif weights[j] < 0:
                    weights[j] += lambda_reg
                else:
                    weights[j] = 0

            weights -= lr * gradients

        # Evaluate with MSE
        y_pred_test = X_test_Scaled.dot(weights[1:]) + weights[0]
        mse = np.mean((y_pred_test - y_test) ** 2)

        if mse < best_mse:
            best_mse = mse
            best_lr = lr
            best_lambda = lambda_reg
            best_weights = weights

print(f"Best Learning Rate: {best_lr}, Best Lambda: {best_lambda} with MSE: {best_mse:.4f}")

# Test prediction and evaluation on the best model
y_pred_test = X_test_Scaled.dot(best_weights[1:]) + best_weights[0]
mae = mean_absolute_error(y_test, y_pred_test)
r2 = r2_score(y_test, y_pred_test)

print(f"Mean Absolute Error (Test): {mae:.4f}")
print(f"R2 Score (Test): {r2:.4f}")

```
