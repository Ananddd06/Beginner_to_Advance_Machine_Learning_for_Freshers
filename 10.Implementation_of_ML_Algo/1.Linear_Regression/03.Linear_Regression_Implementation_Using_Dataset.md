## Implementation of the Linear Regression using the Pands , Numpy and Math

```python
import numpy as np
import pandas as pd
import math

# Make sure y_train is a column vector
y_train = y_train.reshape(-1, 1)
m, n = X_train_Scaled.shape

# Step 1: Add bias (intercept) to X
X_b = np.c_[np.ones((m, 1)), X_train_Scaled]  # shape: (m, n+1)

# Step 2: Initialize weights (w0 is bias)
weights = np.zeros((n + 1, 1))  # shape: (n+1, 1)

# Step 3: Hyperparameters
learning_rate = 0.01
n_epochs = 1000

# Step 4: Training with Gradient Descent
for epoch in range(n_epochs):
    y_pred = X_b.dot(weights)
    error = y_pred - y_train
    gradients = (2/m) * X_b.T.dot(error)
    weights -= learning_rate * gradients

    if epoch % 100 == 0:
        mse = (error ** 2).mean()
        print(f"Epoch {epoch} | MSE: {mse:.4f}")

# Step 5: Prediction Function
def predict(X_input):
    X_input_b = np.c_[np.ones((X_input.shape[0], 1)), X_input]
    return X_input_b.dot(weights)

# Step 6: Predict on test set
y_pred_test = predict(X_test_Scaled)

# Step 7: Evaluation
from sklearn.metrics import mean_absolute_error, r2_score
mae = mean_absolute_error(y_test, y_pred_test)
r2 = r2_score(y_test, y_pred_test)

print("\nEvaluation on Test Set:")
print(f"Mean Absolute Error: {mae:.4f}")
print(f"R2 Score: {r2:.4f}")

```

## For sample Testing

```python
import numpy as np
import pandas as pd
import math
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

# Step 0: Sample Dataset
df = pd.DataFrame({
    'feature1': [1.1, 2.0, 3.2, 4.5, 5.9],
    'feature2': [3.4, 1.2, 2.2, 0.5, 1.0],
    'target': [10.5, 12.3, 15.2, 18.5, 20.3]
})

# Splitting features and target
X = df[['feature1', 'feature2']].values
y = df['target'].values

# Step 1: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Feature Scaling (Standardization)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_Scaled = scaler.fit_transform(X_train)
X_test_Scaled = scaler.transform(X_test)

# Step 3: Prepare y_train
y_train = y_train.reshape(-1, 1)
m, n = X_train_Scaled.shape

# Step 4: Add Bias term
X_b = np.c_[np.ones((m, 1)), X_train_Scaled]

# Step 5: Initialize weights
weights = np.zeros((n + 1, 1))

# Step 6: Set Hyperparameters
learning_rate = 0.01
n_epochs = 1000

# Step 7: Train with Gradient Descent
for epoch in range(n_epochs):
    y_pred = X_b.dot(weights)
    error = y_pred - y_train
    gradients = (2/m) * X_b.T.dot(error)
    weights -= learning_rate * gradients

    if epoch % 100 == 0:
        mse = (error ** 2).mean()
        print(f"Epoch {epoch} | MSE: {mse:.4f}")

# Step 8: Prediction function
def predict(X_input):
    X_input_b = np.c_[np.ones((X_input.shape[0], 1)), X_input]
    return X_input_b.dot(weights)

# Step 9: Predict on Test Set
y_pred_test = predict(X_test_Scaled)

# Step 10: Evaluation
mae = mean_absolute_error(y_test, y_pred_test)
r2 = r2_score(y_test, y_pred_test)

print("\nEvaluation on Test Set:")
print(f"Mean Absolute Error: {mae:.4f}")
print(f"R2 Score: {r2:.4f}")

```

# Linear Regression and Regularization (Ridge and Lasso)

## 1. Linear Regression

We aim to find the weights **w** that minimize the Mean Squared Error (MSE):

$$
\hat{y} = Xw + b
$$

Where:

- \( \hat{y} \) is the predicted value
- \( X \) is the feature matrix
- \( w \) is the vector of weights (including bias)

The cost function (loss) is the Mean Squared Error (MSE):

$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( y^{(i)} - \hat{y}^{(i)} \right)^2
$$

Where:

- \( m \) is the number of training examples
- \( y^{(i)} \) is the actual value for the \(i\)-th example
- \( \hat{y}^{(i)} \) is the predicted value for the \(i\)-th example

The solution to linear regression using the **Normal Equation** (closed-form solution) is:

$$
w = (X^T X)^{-1} X^T y
$$

If we include the bias term \( b \), we augment \( X \) with a column of 1s, making \( X_b \):

$$
X_b = \left[\begin{array}{c} 1 \\ X \end{array}\right]
$$

---

## 2. Ridge Regression (L2 Regularization)

Ridge regression introduces **L2 regularization** to prevent overfitting by adding a penalty term to the cost function.

The cost function becomes:

$$
J(w) = \frac{1}{2m} \sum_{i=1}^{m} \left( y^{(i)} - \hat{y}^{(i)} \right)^2 + \lambda \sum_{j=1}^{n} w_j^2
$$

Where:

- \( \lambda \) is the regularization strength (hyperparameter)
- \( w_j \) is the weight of the \(j\)-th feature

The solution for **Ridge Regression** becomes:

$$
w = (X^T X + \lambda I)^{-1} X^T y
$$

Where:

- \( I \) is the identity matrix (size \( n \times n \))
- We exclude the bias term \( b \) from regularization (i.e., set \( I[0][0] = 0 \))

---

## 3. Lasso Regression (L1 Regularization)

Lasso regression adds **L1 regularization** to the cost function, encouraging sparsity (i.e., some weights may become zero, effectively eliminating features).

The cost function becomes:

$$
J(w) = \frac{1}{2m} \sum_{i=1}^{m} \left( y^{(i)} - \hat{y}^{(i)} \right)^2 + \lambda \sum_{j=1}^{n} |w_j|
$$

Where:

- \( \lambda \) is the regularization strength (hyperparameter)
- \( |w_j| \) is the absolute value of the weight of the \(j\)-th feature

### Important:

There is **no closed-form solution** for Lasso regression. It requires **iterative methods** like:

- **Coordinate Descent**
- **Subgradient Descent**

---

## Summary

- **Linear Regression** uses the closed-form solution via the Normal Equation to minimize the cost function (MSE).
- **Ridge Regression** (L2 Regularization) introduces a penalty term to the cost function to prevent overfitting by penalizing large weights.
- **Lasso Regression** (L1 Regularization) similarly adds a penalty term but encourages sparse solutions where some weights are driven to zero.
