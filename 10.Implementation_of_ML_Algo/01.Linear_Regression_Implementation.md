# Implementation of the Linear Regression

- PYTHON CODE

```python

class LinearRegression:
    def __init__(self, alpha=0.1, iterations=1000):
        self.alpha = alpha  # Learning rate
        self.iterations = iterations  # Number of iterations
        self.beta_0 = 0  # Intercept (bias term)
        self.beta_1 = 0  # Slope (coefficient)

    def compute_cost(self, X, y):
        m = len(y)
        predictions = self.predict(X)
        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
        return cost

    def predict(self, X):
        return self.beta_0 + self.beta_1 * X

    def gradient_descent(self, X, y):
        m = len(y)
        cost_history = []

        for _ in range(self.iterations):
            predictions = self.predict(X)

            # Compute gradients
            beta_0_grad = (1 / m) * np.sum(predictions - y)
            beta_1_grad = (1 / m) * np.sum((predictions - y) * X)

            # Update parameters
            self.beta_0 -= self.alpha * beta_0_grad
            self.beta_1 -= self.alpha * beta_1_grad

            # Save cost for plotting
            cost_history.append(self.compute_cost(X, y))

        return cost_history

    def fit(self, X, y):
        # This is where gradient descent is called and the model is trained
        cost_history = self.gradient_descent(X, y)
        return cost_history

    def get_parameters(self):
        return self.beta_0, self.beta_1


# Create LinearRegression instance and train the model
model = LinearRegression(alpha=0.1, iterations=1000)
cost_history = model.fit(X, y)  # Calling fit() to train the model


# Output the parameters
beta_0, beta_1 = model.get_parameters()
print(f"Intercept (beta_0): {beta_0}")
print(f"Slope (beta_1): {beta_1}")

```

# **Implementation of Linear Regression** ğŸ“ˆ

### **Code Explanation** ğŸ“

This code implements a basic **Linear Regression** model using **Gradient Descent** for optimization.

### **Class: LinearRegression** ğŸ«

- **`__init__(self, alpha=0.1, iterations=1000)`**:

  - Initializes the model with a **learning rate (`alpha`)** and the number of **iterations** for the gradient descent process.
  - **`beta_0`** is the intercept (bias term), and **`beta_1`** is the slope (coefficient).
  - **Default values**: `alpha = 0.1` and `iterations = 1000`.

- **`compute_cost(self, X, y)`**: ğŸ“Š

  - Calculates the **cost function** (mean squared error) based on the difference between the predicted and actual values.
  - This helps in measuring how well the model is fitting the data.

  **Formula**:

  $$
  J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
  $$

- **`predict(self, X)`**: ğŸ”®

  - Makes predictions by using the linear equation:
    $$
    \hat{y} = \beta_0 + \beta_1 \cdot X
    $$

- **`gradient_descent(self, X, y)`**: ğŸ§ 

  - **Performs the gradient descent** algorithm to update the parameters (`beta_0`, `beta_1`) iteratively:
    - Computes the gradients for **intercept** (`beta_0`) and **slope** (`beta_1`).
    - Updates the parameters by subtracting the product of the learning rate and the gradients from the current parameters.
    - **Records the cost** at each iteration to visualize how the model improves.

- **`fit(self, X, y)`**: ğŸ”§

  - **Trains the model** by calling the `gradient_descent()` method.
  - Returns the **cost history** over the iterations to track the training progress.

- **`get_parameters(self)`**: ğŸ“
  - Returns the trained model parameters: **intercept** (`beta_0`) and **slope** (`beta_1`).

### **Using the Model** ğŸ› ï¸

1. **Creating the Model Instance**:

   - `model = LinearRegression(alpha=0.1, iterations=1000)` creates an instance of the `LinearRegression` class.
   - `alpha` is the learning rate, and `iterations` controls how many times the gradient descent will run.

2. **Training the Model**:

   - `cost_history = model.fit(X, y)` calls the `fit()` method to train the model with the given **X** (features) and **y** (target labels).
   - The `fit()` method runs the gradient descent and returns the **cost history** (the value of the cost function at each iteration).

3. **Getting the Parameters**:
   - `beta_0, beta_1 = model.get_parameters()` retrieves the **intercept** (`beta_0`) and **slope** (`beta_1`) of the trained model.
   - These values represent the **best-fit line** for the data.

### **Output** ğŸ¯

- The **intercept** (`beta_0`) and **slope** (`beta_1`) are printed:
  ```python
  print(f"Intercept (beta_0): {beta_0}")
  print(f"Slope (beta_1): {beta_1}")
  ```
