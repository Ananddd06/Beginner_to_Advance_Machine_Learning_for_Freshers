# Performance Metrics: R-squared & Adjusted R-squared ğŸ’¡

In regression analysis, **R-squared** (RÂ²) and **Adjusted R-squared** are two important metrics used to assess the performance of the model. Letâ€™s dive deeper into both of them and explore when **Adjusted R-squared** is the better metric to use. ğŸš€ These metrics are particularly used for **regression problems**, where the target variable is continuous. They are widely used in **linear regression**, **random forest regression**, and other regression models. ğŸ¯

## R-squared (RÂ²) ğŸ§®

### What is R-squared? ğŸ“Š

**R-squared** is a metric that measures the proportion of the variance in the dependent variable (**y**) that is explained by the independent variables (**xâ‚, xâ‚‚, ..., xn**) in the model.

The formula for **R-squared** is:

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

Where:

- \( y_i \) is the actual value of the target (dependent variable).
- \( \hat{y}\_i \) is the predicted value.
- \( \bar{y} \) is the mean of the actual values.
- \( n \) is the number of data points.

### Interpretation of R-squared ğŸ¯

- **RÂ² = 1**: The model perfectly predicts the data (no error).
- **RÂ² = 0**: The model does not explain any of the variance in the target variable and performs as well as predicting the mean for all data points.

**R-squared** gives us a quick measure of how well our model fits the data, but it has some limitations. One of the main issues is that **R-squared can increase when you add more independent features**, even if those features are not useful for predicting the target variable. ğŸ¤”

### Example:

If you add a **non-correlated feature** to your model (a feature that doesn't actually contribute to explaining the target variable), **R-squared** will likely **increase**, even though the model's performance has not improved. This can lead to an overestimation of how good the model is. ğŸ˜¬

---

## Adjusted R-squared (RÂ² Adjusted) ğŸ“

### What is Adjusted R-squared? ğŸ§‘â€ğŸ’»

**Adjusted R-squared** adjusts the R-squared value to account for the number of independent variables in the model. It penalizes the addition of irrelevant features and provides a more accurate measure of model performance, especially when comparing models with different numbers of features. âš–ï¸

The formula for **Adjusted R-squared** is:

$$
R^2_{\text{adj}} = 1 - \left(1 - R^2\right) \frac{n - 1}{n - p - 1}
$$

Where:

- \( R^2 \) is the R-squared value.
- \( n \) is the number of data points.
- \( p \) is the number of independent variables (predictors).

### Why is Adjusted R-squared Better? ğŸ¤©

While **R-squared** can sometimes give a misleading view of model quality by increasing with the addition of non-correlated features, **Adjusted R-squared** takes into account the **number of predictors** in the model. This makes it a more reliable metric, particularly when comparing models with different numbers of features.

#### Key Features of Adjusted R-squared:

- **Penalizes additional predictors**: If you add more independent variables that donâ€™t improve the model, the **Adjusted RÂ²** will **decrease**.
- **Corrects for overfitting**: By accounting for the number of predictors, it prevents the model from appearing better than it actually is when adding irrelevant features.
- **More accurate for model comparison**: When you have multiple models with different numbers of predictors, **Adjusted RÂ²** gives a fairer comparison.

### Interpretation of Adjusted R-squared ğŸ¯

- **Adjusted RÂ² is high**: The model explains a large proportion of the variance in the target variable, while using an appropriate number of predictors.
- **Adjusted RÂ² decreases** when adding irrelevant features, indicating overfitting.

---

## Key Takeaways ğŸ“š

1. **R-squared** is a useful metric for understanding how well the model fits the data, but it can be misleading when adding more features (especially non-correlated ones).
2. **Adjusted R-squared** is a more reliable metric because it adjusts for the number of independent variables and penalizes overfitting.
3. When comparing models, especially those with different numbers of predictors, **Adjusted R-squared** is the better metric to use for **evaluating model performance**. âœ…

---

## Conclusion ğŸ

In summary, while **R-squared** can give you an idea of how well the model fits the data, **Adjusted R-squared** is the more robust and reliable metric, particularly when comparing models with different numbers of independent features. It ensures that adding more features only increases the model's quality when those features are genuinely valuable. ğŸš€

These metrics are particularly used in **regression problems** where the target variable is continuous. They are widely used in **linear regression**, **random forest regression**, and other regression models to assess how well the model is predicting continuous data points. ğŸ“Š

Happy modeling! ğŸ’»ğŸ’¡
