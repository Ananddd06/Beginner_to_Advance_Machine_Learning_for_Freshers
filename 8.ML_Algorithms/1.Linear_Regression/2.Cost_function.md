# üìâ **Cost Function in Linear Regression**

In **Linear Regression**, our goal is to find the **best-fit line** that predicts the dependent variable \( y \) based on the independent variable \( x \). The best-fit line is determined by adjusting the parameters \( \theta_0 \) (intercept) and \( \theta_1 \) (slope) to minimize the **error** between the predicted and actual values.

To achieve this, we use a **cost function** that helps us quantify the error in our predictions. The idea is to minimize this cost function so that the line we draw represents the best fit for the given data.

---

## üßë‚Äçüè´ **The Cost Function Formula:**

The cost function is defined as:

$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2
$$

Where:

- \( J(\theta_0, \theta_1) \) is the **cost function**, which measures the error between the predicted and actual values.
- \( m \) is the number of training examples (data points).
- \( h\_{\theta}(x^{(i)}) = \theta_0 + \theta_1 \cdot x^{(i)} \) is the **hypothesis function** (predicted value) for the \( i \)-th training example.
- \( y^{(i)} \) is the **actual value** of the dependent variable for the \( i \)-th example.

---

## ‚ö° **Objective:**

- The objective is to **minimize** the cost function \( J(\theta_0, \theta_1) \) by adjusting the values of \( \theta_0 \) and \( \theta_1 \).
- As we move towards the **global minimum** of the cost function, we can achieve the best fit line that **minimizes the error** between the predicted and actual values.

---

## üîç **Example:**

Let‚Äôs work through a simple example with 3 data points to calculate the **cost function**.

#### Given Data:

- **x values**: [1, 2, 3]
- **y values**: [2, 3, 4]

We will initialize the parameters \( \theta_0 = 0 \) and try different values for \( \theta_1 \): \( 1 \), \( 0.5 \), and \( 0 \).

---

### **Step-by-Step Calculation:**

#### **For \( \theta_0 = 0 \) and \( \theta_1 = 1 \):**

The **hypothesis function** (predicted value) is:

$$
h_{\theta}(x) = 0 + 1 \cdot x = x
$$

So for each data point:

- For \( x = 1 \):  
  The predicted value \( h\_{\theta}(1) = 1 \)  
  The actual value \( y = 2 \)  
  **Error**: \( (1 - 2)^2 = (-1)^2 = 1 \)

- For \( x = 2 \):  
  The predicted value \( h\_{\theta}(2) = 2 \)  
  The actual value \( y = 3 \)  
  **Error**: \( (2 - 3)^2 = (-1)^2 = 1 \)

- For \( x = 3 \):  
  The predicted value \( h\_{\theta}(3) = 3 \)  
  The actual value \( y = 4 \)  
  **Error**: \( (3 - 4)^2 = (-1)^2 = 1 \)

Now, sum all the squared errors:

$$
J(\theta_0, \theta_1) = \frac{1}{2 \cdot 3} \left( 1 + 1 + 1 \right) = \frac{3}{6} = 0.5
$$

So, for \( \theta_0 = 0 \) and \( \theta_1 = 1 \), the total cost (error) is **0.5**.

---

#### **For \( \theta_0 = 0 \) and \( \theta_1 = 0.5 \):**

The **hypothesis function** becomes:

$$
h_{\theta}(x) = 0 + 0.5 \cdot x = 0.5x
$$

For each data point:

- For \( x = 1 \):  
  The predicted value \( h\_{\theta}(1) = 0.5 \)  
  The actual value \( y = 2 \)  
  **Error**: \( (0.5 - 2)^2 = (-1.5)^2 = 2.25 \)

- For \( x = 2 \):  
  The predicted value \( h\_{\theta}(2) = 1 \)  
  The actual value \( y = 3 \)  
  **Error**: \( (1 - 3)^2 = (-2)^2 = 4 \)

- For \( x = 3 \):  
  The predicted value \( h\_{\theta}(3) = 1.5 \)  
  The actual value \( y = 4 \)  
  **Error**: \( (1.5 - 4)^2 = (-2.5)^2 = 6.25 \)

Now, sum all the squared errors:

$$
J(\theta_0, \theta_1) = \frac{1}{2 \cdot 3} \left( 2.25 + 4 + 6.25 \right) = \frac{12.5}{6} = 2.0833
$$

So, for \( \theta_0 = 0 \) and \( \theta_1 = 0.5 \), the total cost (error) is **2.0833**.

---

#### **For \( \theta_0 = 0 \) and \( \theta_1 = 0 \):**

The **hypothesis function** becomes:

$$
h_{\theta}(x) = 0 + 0 \cdot x = 0
$$

For each data point:

- For \( x = 1 \):  
  The predicted value \( h\_{\theta}(1) = 0 \)  
  The actual value \( y = 2 \)  
  **Error**: \( (0 - 2)^2 = (-2)^2 = 4 \)

- For \( x = 2 \):  
  The predicted value \( h\_{\theta}(2) = 0 \)  
  The actual value \( y = 3 \)  
  **Error**: \( (0 - 3)^2 = (-3)^2 = 9 \)

- For \( x = 3 \):  
  The predicted value \( h\_{\theta}(3) = 0 \)  
  The actual value \( y = 4 \)  
  **Error**: \( (0 - 4)^2 = (-4)^2 = 16 \)

Now, sum all the squared errors:

$$
J(\theta_0, \theta_1) = \frac{1}{2 \cdot 3} \left( 4 + 9 + 16 \right) = \frac{29}{6} = 4.8333
$$

So, for \( \theta_0 = 0 \) and \( \theta_1 = 0 \), the total cost (error) is **4.8333**.

---

## üìä **Interpreting the Results:**

- We see that the lowest cost occurs when \( \theta_0 = 0 \) and \( \theta_1 = 1 \), indicating that this combination gives the **best fit line**.
- The values for \( \theta_1 = 0.5 \) and \( \theta_1 = 0 \) result in higher costs, showing that the model is not a good fit for this data.

---

## üöÄ **Gradient Descent and the Global Minimum:**

In linear regression, we aim to **minimize** the cost function \( J(\theta_0, \theta_1) \), not maximize it. The **gradient descent** algorithm helps us find the **global minimum** of the cost function, which corresponds to the best-fit line.

- **Global Minimum**: The point where the cost function is at its lowest, indicating the best values for \( \theta_0 \) and \( \theta_1 \) that result in the smallest error between predicted and actual values.
- **Gradient Descent**: The process of iteratively adjusting \( \theta_0 \) and \( \theta_1 \) to reach this **global minimum**.

In simple terms, the goal of gradient descent is to move **down the curve** to find the global minimum, which results in the **best-fit line** for the data. Maximizing the cost function would only worsen the model, so gradient descent seeks to minimize the error by finding that global minimum.

---

## üöÄ **Conclusion:**

- The cost function helps us **quantify the error** in our model. By **minimizing the cost**, we can find the best-fit line, which is the line that best represents the relationship between \( x \) and \( y \).
- In this example, we learned that adjusting the values of \( \theta_0 \) and \( \theta_1 \) helps us find the optimal parameters for the best-fit line.

Happy learning, and keep practicing with different values of \( \theta_0 \) and \( \theta_1 \) to better understand the process of Linear Regression! üéâ
