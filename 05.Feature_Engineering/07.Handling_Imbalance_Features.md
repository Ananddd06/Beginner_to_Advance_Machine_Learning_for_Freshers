# ⚖️ Handling Imbalanced Features in Machine Learning

Imbalanced features—where certain categories or classes dominate others—can lead to **biased models**, poor generalization, and incorrect conclusions. As an Applied Scientist at scale (e.g., Amazon or enterprise-grade systems), it’s crucial to detect and treat imbalance appropriately.

---

## 🧠 What Is Feature Imbalance?

Feature imbalance refers to **uneven distributions** in either:

1. **Target Variable (Class Imbalance)** – e.g., 95% ‘No’ vs 5% ‘Yes’ for fraud
2. **Categorical Feature Imbalance** – e.g., 98% users from one city

> **Goal:** Prevent the model from being biased toward majority categories/classes.

---

## 🔍 1. Detecting Imbalanced Features

### 📊 Frequency Table

```python
df['feature'].value_counts(normalize=True)
```

### 📉 Visual Inspection

- **Bar Plots**
- **Pie Charts**
- **Seaborn Countplot**

---

## 📦 2. Strategies for Handling Class Imbalance (Target Variable)

### 🔹 A. **Resampling Techniques**

#### ➤ Undersampling (Reduce Majority Class)

```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler()
X_res, y_res = rus.fit_resample(X, y)
```

✅ Fast and simple
❌ Risk of losing useful data

#### ➤ Oversampling (Boost Minority Class)

```python
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler()
X_res, y_res = ros.fit_resample(X, y)
```

✅ Preserves all data
❌ Can lead to overfitting

#### ➤ SMOTE (Synthetic Minority Oversampling)

```python
from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_res, y_res = smote.fit_resample(X, y)
```

✅ Generates synthetic data using nearest neighbors

#### ➤ SMOTENC (For categorical + numerical data)

```python
from imblearn.over_sampling import SMOTENC
smote_nc = SMOTENC(categorical_features=[0, 2], random_state=42)
```

---

### 🔹 B. **Class Weights (Algorithm-Level Tuning)**

Most classifiers accept `class_weight` parameter:

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(class_weight='balanced')
```

✅ Keeps full dataset
✅ Works well with tree-based or linear models

---

### 🔹 C. **Threshold Moving / Calibration**

- Shift decision threshold to favor minority class
- Use `precision-recall curve` or `ROC curve` to optimize threshold

---

## 📊 3. Handling Imbalanced Categorical Features

When features are skewed (e.g., 99% users from ‘India’, 1% from other countries):

### 🔸 A. **Combine Rare Categories**

```python
threshold = 100
top_categories = df['feature'].value_counts()[df['feature'].value_counts() > threshold].index
df['feature'] = df['feature'].apply(lambda x: x if x in top_categories else 'Other')
```

### 🔸 B. **Frequency Encoding**

Replace categories with frequency counts:

```python
freq = df['feature'].value_counts()
df['feature_encoded'] = df['feature'].map(freq)
```

### 🔸 C. **Target Encoding with Caution**

Rare categories can overfit on target encoding; use regularization or cross-validation.

---

## 🧠 Real-World Example (Amazon Style)

> At Amazon, in fraud detection, legitimate transactions heavily outnumber fraudulent ones. We combine **SMOTE** for balancing and **class-weighted XGBoost** to optimize recall for the minority class. For rare user attributes (e.g., device types), we group them into 'Other' categories and monitor drift over time.

---

## ✅ Best Practices

- Don’t blindly balance — understand why imbalance exists
- Use **cross-validation** when applying target encoding
- Prefer **class weights** over resampling if data is limited
- Analyze **confusion matrix, precision, recall, F1** to evaluate performance

---

## 🛠 Tools & Libraries

- `imblearn` (oversampling, SMOTE)
- `scikit-learn` (`class_weight`)
- `xgboost`, `lightgbm` (support imbalance natively)
- `seaborn`, `matplotlib` (visual inspection)

---

Addressing imbalance is about ensuring **fairness and accuracy** in your models. Whet
