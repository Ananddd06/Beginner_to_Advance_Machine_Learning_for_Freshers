# âš–ï¸ Handling Imbalanced Features in Machine Learning

Imbalanced featuresâ€”where certain categories or classes dominate othersâ€”can lead to **biased models**, poor generalization, and incorrect conclusions. As an Applied Scientist at scale (e.g., Amazon or enterprise-grade systems), itâ€™s crucial to detect and treat imbalance appropriately.

---

## ðŸ§  What Is Feature Imbalance?

Feature imbalance refers to **uneven distributions** in either:

1. **Target Variable (Class Imbalance)** â€“ e.g., 95% â€˜Noâ€™ vs 5% â€˜Yesâ€™ for fraud
2. **Categorical Feature Imbalance** â€“ e.g., 98% users from one city

> **Goal:** Prevent the model from being biased toward majority categories/classes.

---

## ðŸ” 1. Detecting Imbalanced Features

### ðŸ“Š Frequency Table

```python
df['feature'].value_counts(normalize=True)
```

### ðŸ“‰ Visual Inspection

- **Bar Plots**
- **Pie Charts**
- **Seaborn Countplot**

---

## ðŸ“¦ 2. Strategies for Handling Class Imbalance (Target Variable)

### ðŸ”¹ A. **Resampling Techniques**

#### âž¤ Undersampling (Reduce Majority Class)

```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler()
X_res, y_res = rus.fit_resample(X, y)
```

âœ… Fast and simple
âŒ Risk of losing useful data

#### âž¤ Oversampling (Boost Minority Class)

```python
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler()
X_res, y_res = ros.fit_resample(X, y)
```

âœ… Preserves all data
âŒ Can lead to overfitting

#### âž¤ SMOTE (Synthetic Minority Oversampling)

```python
from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_res, y_res = smote.fit_resample(X, y)
```

âœ… Generates synthetic data using nearest neighbors

#### âž¤ SMOTENC (For categorical + numerical data)

```python
from imblearn.over_sampling import SMOTENC
smote_nc = SMOTENC(categorical_features=[0, 2], random_state=42)
```

---

### ðŸ”¹ B. **Class Weights (Algorithm-Level Tuning)**

Most classifiers accept `class_weight` parameter:

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(class_weight='balanced')
```

âœ… Keeps full dataset
âœ… Works well with tree-based or linear models

---

### ðŸ”¹ C. **Threshold Moving / Calibration**

- Shift decision threshold to favor minority class
- Use `precision-recall curve` or `ROC curve` to optimize threshold

---

## ðŸ“Š 3. Handling Imbalanced Categorical Features

When features are skewed (e.g., 99% users from â€˜Indiaâ€™, 1% from other countries):

### ðŸ”¸ A. **Combine Rare Categories**

```python
threshold = 100
top_categories = df['feature'].value_counts()[df['feature'].value_counts() > threshold].index
df['feature'] = df['feature'].apply(lambda x: x if x in top_categories else 'Other')
```

### ðŸ”¸ B. **Frequency Encoding**

Replace categories with frequency counts:

```python
freq = df['feature'].value_counts()
df['feature_encoded'] = df['feature'].map(freq)
```

### ðŸ”¸ C. **Target Encoding with Caution**

Rare categories can overfit on target encoding; use regularization or cross-validation.

---

## ðŸ§  Real-World Example (Amazon Style)

> At Amazon, in fraud detection, legitimate transactions heavily outnumber fraudulent ones. We combine **SMOTE** for balancing and **class-weighted XGBoost** to optimize recall for the minority class. For rare user attributes (e.g., device types), we group them into 'Other' categories and monitor drift over time.

---

## âœ… Best Practices

- Donâ€™t blindly balance â€” understand why imbalance exists
- Use **cross-validation** when applying target encoding
- Prefer **class weights** over resampling if data is limited
- Analyze **confusion matrix, precision, recall, F1** to evaluate performance

---

## ðŸ›  Tools & Libraries

- `imblearn` (oversampling, SMOTE)
- `scikit-learn` (`class_weight`)
- `xgboost`, `lightgbm` (support imbalance natively)
- `seaborn`, `matplotlib` (visual inspection)

---

Addressing imbalance is about ensuring **fairness and accuracy** in your models. Whet
