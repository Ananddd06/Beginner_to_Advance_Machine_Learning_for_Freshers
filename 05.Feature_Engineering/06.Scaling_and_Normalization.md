# ⚖️ Feature Scaling and Normalization in Machine Learning

Feature scaling (or normalization) ensures that **numerical features contribute equally** to model learning and optimization. As an Applied Scientist at Amazon-level systems, we never skip this step, especially when using gradient-based or distance-sensitive algorithms.

---

## 🧠 Why Scale Features?

### 🚫 Problem Without Scaling

- Features with larger magnitudes dominate loss or distance computations.
- Gradient descent converges slowly or gets stuck.
- Distance-based algorithms (like KNN, SVM, K-Means) become biased.

### ✅ Benefits of Scaling

- Improves **model performance and convergence**
- Makes coefficients more **interpretable** in linear models
- Ensures **fair feature contributions** in distance-based algorithms

---

## 🔢 1. Types of Feature Scaling Techniques

### 🔹 A. **Standardization (Z-score Normalization)**

Transforms features to have **zero mean and unit variance**:
$x' = \frac{x - \mu}{\sigma}$

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)
```

✅ Use when data is normally distributed or when using **linear models, SVMs, logistic regression**

---

### 🔹 B. **Min-Max Scaling (Normalization)**

Scales data to a **fixed range** \[0, 1]:
$x' = \frac{x - x_{min}}{x_{max} - x_{min}}$

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df)
```

✅ Ideal for **neural networks** or when input features need to be bounded

---

### 🔹 C. **Robust Scaling**

Uses **median and IQR** instead of mean/std (good for outliers):

```python
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
df_scaled = scaler.fit_transform(df)
```

✅ Use when data contains **outliers**

---

### 🔹 D. **MaxAbs Scaling**

Scales data in the range \[-1, 1] using maximum absolute value:

```python
from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler()
df_scaled = scaler.fit_transform(df)
```

✅ Great for **sparse data (e.g., TF-IDF features)**

---

### 🔹 E. **Unit Vector Scaling (Normalization to L2 norm)**

Makes every data point (row) have unit norm:

```python
from sklearn.preprocessing import Normalizer
scaler = Normalizer(norm='l2')
df_scaled = scaler.fit_transform(df)
```

✅ Used in **text classification**, **cosine similarity**, **recommendation systems**

---

## 🧪 When to Use Which Scaling

| Technique      | Use Case                                    |
| -------------- | ------------------------------------------- |
| StandardScaler | Linear regression, logistic regression, SVM |
| MinMaxScaler   | Deep learning, image data                   |
| RobustScaler   | Data with outliers                          |
| MaxAbsScaler   | Sparse features (e.g., TF-IDF)              |
| Normalizer     | Row-wise scaling for similarity tasks       |

---

## 📊 Scaling vs Normalization

| Concept       | Description                                   |
| ------------- | --------------------------------------------- |
| Scaling       | Adjusting the **range** of feature values     |
| Normalization | Adjusting the **distribution** (e.g., norm 1) |

---

## ⚠️ Scaling Caveats

- **Always scale after train-test split** or use `fit` on train and `transform` on test
- **Don’t scale categorical variables** or identifiers
- **Scale after handling missing values** and **before PCA** or distance-based algorithms

---

## 🧠 Real-World Example (Amazon Style)

> In our recommendation systems, when using cosine similarity for user vectors, we normalize each row to unit norm. For logistic models predicting click-through rates, we standardize to zero mean/unit variance to stabilize training.

---

## ✅ Final Checklist

- [ ] Choose scaler based on feature distribution and algorithm
- [ ] Fit only on training data to avoid leakage
- [ ] Store the scaler if deploying a model

---

Feature scaling is **not optional**—it’s foundational for ensuring your models behave as expected. Know your data, pick the right scaler, and apply it consistently!
