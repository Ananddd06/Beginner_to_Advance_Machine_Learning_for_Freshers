# ðŸ“Š Feature Engineering: Complete Guide

Feature engineering is one of the **most critical steps** in building high-performing machine learning models. It involves transforming raw data into meaningful inputs that help models learn patterns better.

---

## ðŸ”¹ 1. Understanding the Data

- **Data Types**: Identify numerical, categorical, text, image, date, etc.
- **Domain Knowledge**: Understand what features mean and their significance.
- **Target Leakage**: Avoid including information from the future or derived from the target.

---

## ðŸ”¹ 2. Handling Missing Values

- **Numerical**: Fill with mean, median, or a constant (e.g., -999).
- **Categorical**: Fill with mode or â€˜Unknownâ€™.
- **Advanced**: Use models (e.g., KNN, MICE) to impute missing values.

---

## ðŸ”¹ 3. Handling Outliers

Techniques:

- Z-score
- IQR method
- Domain thresholds

Actions:

- Cap, remove, or transform (e.g., log scaling).

---

## ðŸ”¹ 4. Encoding Categorical Features

- **Label Encoding**: For ordinal data.
- **One-Hot Encoding**: For nominal data with fewer unique values.
- **Target Encoding**: Replace category with average of the target.
- **Frequency/Count Encoding**: Replace with how often each category appears.

---

## ðŸ”¹ 5. Feature Scaling / Normalization

- **StandardScaler** (Z-score): $x' = \frac{x - \mu}{\sigma}$
- **MinMaxScaler**: Rescales to 0â€“1 range.
- **RobustScaler**: Based on median and IQR (handles outliers better).
- **Log/Box-Cox/Yeo-Johnson Transform**: For skewed data.

---

## ðŸ”¹ 6. Creating New Features

- **Polynomial Features**: $x^2, x^3, x_1 \times x_2$
- **Ratios**: e.g., `income / age`
- **Aggregations**: Group-wise mean, sum, count, etc.
- **Date-Time Features**: Extract day, month, hour, is_weekend, etc.
- **Domain Features**: Custom features based on problem context.

---

## ðŸ”¹ 7. Dimensionality Reduction

- **PCA (Principal Component Analysis)**
- **t-SNE / UMAP** (for visualization)

### Feature Selection:

- **Filter**: Correlation, Variance Threshold
- **Wrapper**: Recursive Feature Elimination (RFE)
- **Embedded**: Lasso, Tree feature importance

---

## ðŸ”¹ 8. Text Feature Engineering

- **Basic**: Word count, sentence count, average word length
- **NLP-based**:

  - TF-IDF
  - Bag of Words
  - Word Embeddings (Word2Vec, GloVe, BERT)

---

## ðŸ”¹ 9. Interaction Features

Combine two or more features to capture interactions:

- Example: `price_per_sqft = price / area`
- `has_children & married` â†’ useful interaction

---

## ðŸ”¹ 10. Feature Binning / Discretization

- **Equal-width Binning**
- **Quantile Binning**
- **Custom Bins** based on domain knowledge
- Used to reduce noise and make models more robust.

---

## ðŸ”¹ 11. Handling Imbalanced Features

- **Rare category grouping**: Combine rare categories into 'Other'
- **Undersampling / Oversampling**: For imbalanced classification

---

## ðŸ”¹ 12. Feature Importance Evaluation

- Use models like Random Forest, XGBoost to evaluate feature importance
- Use SHAP or LIME for interpretability

---

## ðŸ”¹ 13. Feature Selection Techniques

- **Correlation analysis**
- **Mutual Information**
- **Chi-Square Test** (for categorical)
- **ANOVA F-test** (for numerical vs categorical)

---

## âœ¨ Bonus: Tools & Libraries for Feature Engineering

- **Pandas / NumPy** â€“ Core transformations
- **Scikit-learn** â€“ Preprocessing
- **Feature-engine** â€“ Specialized feature engineering
- **Category Encoders** â€“ Advanced encoders
- **Polars** â€“ High-performance data processing

---

This markdown file serves as a complete reference guide for Feature Engineering. It can be adapted into a cheat sheet, training module, or project checklist.
